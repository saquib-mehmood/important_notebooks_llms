{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1558d35-29c0-4fc5-bdac-ae5f0d90f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PWD=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\ \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET THIRD_PARTY=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET TVM_LIBRARY_PATH=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PATH=C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\Library\\mingw-w64\\bin;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\Library\\usr\\bin;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\Library\\bin;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\Scripts;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\bin;C:\\Users\\MicroZaib\\miniconda3\\condabin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\JupyterLab;C:\\Users\\MicroZaib\\AppData\\Local\\Programs\\Python\\Python312\\Scripts;C:\\Users\\MicroZaib\\AppData\\Local\\Programs\\Python\\Python312;C:\\Users\\MicroZaib\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\MicroZaib\\miniconda3;C:\\Users\\MicroZaib\\miniconda3\\Scripts;C:\\Users\\MicroZaib\\miniconda3\\Library\\bin;C:\\Users\\MicroZaib\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Windows\\System32;.;C:\\Users\\MicroZaib\\AppData\\Local\\Programs\\Ollama;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\cpp\\;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTORCH_AIE_PATH=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\ \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\python \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\onnx-ops\\python \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\onnx-ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\tools \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\onnx-ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\tools;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ext\\smoothquant\\smoothquant \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>set XRT_PATH=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\xrt-ipu \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>set TARGET_DESIGN= \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>set DEVICE=phx \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>set XLNX_VART_FIRMWARE=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\/xclbin/phx \n"
     ]
    }
   ],
   "source": [
    "#set environment variables after creating environment from the yaml file provided\n",
    "!setup.bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3cfa33-ebdd-4b05-ab72-7cdee6af17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add required system paths\n",
    "import sys\n",
    "sys.path.append(\"../ext\") \n",
    "sys.path.append(\"../smoothquant_package\") \n",
    "sys.path.append(\"./gradio_app\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0004b494-d235-44b1-b6d0-da7bc7aadec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff068bc1-fd9c-4794-a1dd-09a7faf583f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from transformers import set_seed, AutoTokenizer, AutoTokenizer, OPTForCausalLM, AutoModelForCausalLM\n",
    "import pathlib\n",
    "# import smooth\n",
    "import torch\n",
    "import random \n",
    "import string\n",
    "import gradio as gr\n",
    "import time\n",
    "# from modeling_ort_amd import ORTModelForCausalLM\n",
    "from smoothquant_package.smooth import smooth_lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90dd1a35-a907-4825-b700-e81851c79658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Florence-2-base_pretrained_fp32'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to define model path where downloaded pretrained model would be saved\n",
    "model_id = \"microsoft/Florence-2-base\" #path to pretrained model on huggingface hub\n",
    "def model_path_fp32(model_id):\n",
    "    model_name = model_id.rsplit('/')[-1]\n",
    "    out_dir = \"./%s_pretrained_fp32\"%model_name\n",
    "    model_path_fp32 = out_dir  \n",
    "    return model_path_fp32\n",
    "\n",
    "model_path_fp32(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e1b1ee-e5c1-4cb2-ba7b-14ba313bf614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a93073d014c4364849ce467deef01c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MicroZaib\\.cache\\huggingface\\hub\\models--microsoft--Florence-2-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The repository for microsoft/Florence-2-base contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Florence-2-base.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\transformers\\dynamic_module_utils.py:609\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 609\u001b[0m     prev_sig_handler \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39msignal(\u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSIGALRM\u001b[49m, _raise_timeout_error)\n\u001b[0;32m    610\u001b[0m     signal\u001b[38;5;241m.\u001b[39malarm(TIME_OUT_REMOTE_CODE)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'signal' has no attribute 'SIGALRM'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_pretrained(out_dir)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# print(out_dir)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m, in \u001b[0;36mdownload_model\u001b[1;34m(model_id)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Downloads and saves a pretratined model from (e.g. HF) to the specified dir\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Args: \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    model_path_pt - pretrained model path (e.g. facebook/opt-1.3b) \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Output: fp-32 model saved in specified dir \"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model_id\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#extract model name from model path pretrained using python string methods\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#specify a dir for saving the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     out_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_pretrained_fp32\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mmodel_name\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:523\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    521\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 523\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    524\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    525\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    526\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    527\u001b[0m     code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[0;32m    528\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    531\u001b[0m )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:937\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    935\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    936\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n\u001b[1;32m--> 937\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m    942\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\transformers\\dynamic_module_utils.py:625\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[0;32m    622\u001b[0m     signal\u001b[38;5;241m.\u001b[39malarm(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;66;03m# OS which does not support signal.SIGALRM\u001b[39;00m\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe repository for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m contains custom code which must be executed to correctly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload the model. You can inspect the repository content at https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_sig_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: The repository for microsoft/Florence-2-base contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Florence-2-base.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "source": [
    "# download and save model from huggingface model hub\n",
    "def download_model(model_id):\n",
    "    \"\"\" Downloads and saves a pretratined model from (e.g. HF) to the specified dir\n",
    "    Args: \n",
    "    model_path_pt - pretrained model path (e.g. facebook/opt-1.3b) \n",
    "    Output: fp-32 model saved in specified dir \"\"\"\n",
    "    \n",
    "    model_name = model_id.rsplit('/')[-1] #extract model name from model path pretrained using python string methods\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "#specify a dir for saving the model\n",
    "    out_dir = \"./%s_pretrained_fp32\"%model_name\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    model.save_pretrained(out_dir)\n",
    "    # print(out_dir)\n",
    "\n",
    "download_model(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15459984-b515-4495-ba92-edf8eb595690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb5bc9a-307b-4723-bf41-04c893663401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.78.tar.gz (50.2 MB)\n",
      "     ---------------------------------------- 0.0/50.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.2 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/50.2 MB 330.3 kB/s eta 0:02:32\n",
      "     --------------------------------------- 0.0/50.2 MB 245.8 kB/s eta 0:03:24\n",
      "     --------------------------------------- 0.0/50.2 MB 245.8 kB/s eta 0:03:24\n",
      "     --------------------------------------- 0.1/50.2 MB 234.9 kB/s eta 0:03:34\n",
      "     --------------------------------------- 0.1/50.2 MB 270.5 kB/s eta 0:03:06\n",
      "     --------------------------------------- 0.1/50.2 MB 262.6 kB/s eta 0:03:11\n",
      "     --------------------------------------- 0.1/50.2 MB 300.4 kB/s eta 0:02:47\n",
      "     --------------------------------------- 0.2/50.2 MB 364.0 kB/s eta 0:02:18\n",
      "     --------------------------------------- 0.2/50.2 MB 393.8 kB/s eta 0:02:07\n",
      "     --------------------------------------- 0.2/50.2 MB 443.5 kB/s eta 0:01:53\n",
      "     --------------------------------------- 0.3/50.2 MB 478.3 kB/s eta 0:01:45\n",
      "     --------------------------------------- 0.3/50.2 MB 554.9 kB/s eta 0:01:30\n",
      "     --------------------------------------- 0.4/50.2 MB 593.2 kB/s eta 0:01:24\n",
      "     --------------------------------------- 0.5/50.2 MB 640.9 kB/s eta 0:01:18\n",
      "     --------------------------------------- 0.5/50.2 MB 682.7 kB/s eta 0:01:13\n",
      "      -------------------------------------- 0.7/50.2 MB 809.4 kB/s eta 0:01:02\n",
      "      -------------------------------------- 0.7/50.2 MB 826.0 kB/s eta 0:01:00\n",
      "      -------------------------------------- 0.8/50.2 MB 874.4 kB/s eta 0:00:57\n",
      "      -------------------------------------- 0.8/50.2 MB 892.9 kB/s eta 0:00:56\n",
      "      -------------------------------------- 0.9/50.2 MB 888.6 kB/s eta 0:00:56\n",
      "      -------------------------------------- 0.9/50.2 MB 897.9 kB/s eta 0:00:55\n",
      "      -------------------------------------- 1.0/50.2 MB 902.4 kB/s eta 0:00:55\n",
      "      -------------------------------------- 1.1/50.2 MB 928.4 kB/s eta 0:00:53\n",
      "      -------------------------------------- 1.1/50.2 MB 943.6 kB/s eta 0:00:52\n",
      "      --------------------------------------- 1.2/50.2 MB 1.0 MB/s eta 0:00:49\n",
      "     - -------------------------------------- 1.3/50.2 MB 1.0 MB/s eta 0:00:48\n",
      "     - -------------------------------------- 1.4/50.2 MB 1.1 MB/s eta 0:00:47\n",
      "     - -------------------------------------- 1.5/50.2 MB 1.1 MB/s eta 0:00:45\n",
      "     - -------------------------------------- 1.6/50.2 MB 1.1 MB/s eta 0:00:44\n",
      "     - -------------------------------------- 1.7/50.2 MB 1.1 MB/s eta 0:00:43\n",
      "     - -------------------------------------- 1.7/50.2 MB 1.1 MB/s eta 0:00:43\n",
      "     - -------------------------------------- 1.8/50.2 MB 1.2 MB/s eta 0:00:41\n",
      "     - -------------------------------------- 2.0/50.2 MB 1.2 MB/s eta 0:00:40\n",
      "     - -------------------------------------- 2.1/50.2 MB 1.3 MB/s eta 0:00:38\n",
      "     - -------------------------------------- 2.1/50.2 MB 1.3 MB/s eta 0:00:38\n",
      "     - -------------------------------------- 2.2/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.3/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.4/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     - -------------------------------------- 2.5/50.2 MB 1.3 MB/s eta 0:00:37\n",
      "     -- ------------------------------------- 2.7/50.2 MB 1.2 MB/s eta 0:00:41\n",
      "     -- ------------------------------------- 3.5/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 3.5/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 3.6/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 3.7/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.8/50.2 MB 1.5 MB/s eta 0:00:31\n",
      "     --- ------------------------------------ 3.9/50.2 MB 1.5 MB/s eta 0:00:31\n",
      "     --- ------------------------------------ 3.9/50.2 MB 1.5 MB/s eta 0:00:31\n",
      "     --- ------------------------------------ 3.9/50.2 MB 1.5 MB/s eta 0:00:31\n",
      "     --- ------------------------------------ 3.9/50.2 MB 1.4 MB/s eta 0:00:33\n",
      "     --- ------------------------------------ 4.0/50.2 MB 1.4 MB/s eta 0:00:33\n",
      "     --- ------------------------------------ 4.1/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.1/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.2/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.3/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.4/50.2 MB 1.5 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.8/50.2 MB 1.2 MB/s eta 0:00:37\n",
      "     --- ------------------------------------ 4.8/50.2 MB 1.2 MB/s eta 0:00:37\n",
      "     ---- ----------------------------------- 5.5/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 5.6/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 5.6/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 5.7/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 5.8/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 5.9/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 5.9/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 6.0/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 6.1/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 6.1/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 6.2/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 6.2/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ----- ---------------------------------- 6.3/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ----- ---------------------------------- 6.4/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 6.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 6.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 6.6/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 6.7/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 6.7/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 6.7/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ----- ---------------------------------- 6.8/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ----- ---------------------------------- 6.9/50.2 MB 1.4 MB/s eta 0:00:32\n",
      "     ----- ---------------------------------- 7.0/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.0/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.1/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.2/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.2/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.3/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.4/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.4/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ----- ---------------------------------- 7.5/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ------ --------------------------------- 8.0/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ------ --------------------------------- 8.0/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ------ --------------------------------- 8.1/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ------ --------------------------------- 8.2/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ------ --------------------------------- 8.3/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ------ --------------------------------- 8.3/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ------ --------------------------------- 8.4/50.2 MB 1.4 MB/s eta 0:00:31\n",
      "     ------ --------------------------------- 8.5/50.2 MB 1.4 MB/s eta 0:00:30\n",
      "     ------ --------------------------------- 8.6/50.2 MB 1.4 MB/s eta 0:00:30\n",
      "     ------ --------------------------------- 8.6/50.2 MB 1.4 MB/s eta 0:00:30\n",
      "     ------ --------------------------------- 8.8/50.2 MB 1.4 MB/s eta 0:00:30\n",
      "     ------- -------------------------------- 8.8/50.2 MB 1.4 MB/s eta 0:00:30\n",
      "     ------- -------------------------------- 9.0/50.2 MB 1.4 MB/s eta 0:00:30\n",
      "     ------- -------------------------------- 9.1/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.9/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.9/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.9/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.9/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 9.9/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     ------- -------------------------------- 10.0/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     -------- ------------------------------- 10.1/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     -------- ------------------------------- 10.2/50.2 MB 1.4 MB/s eta 0:00:29\n",
      "     -------- ------------------------------- 10.3/50.2 MB 1.5 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 10.4/50.2 MB 1.5 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 10.5/50.2 MB 1.5 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 10.5/50.2 MB 1.5 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 10.6/50.2 MB 1.5 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 10.7/50.2 MB 1.5 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 10.8/50.2 MB 1.5 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 10.9/50.2 MB 1.5 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 10.9/50.2 MB 1.5 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 11.0/50.2 MB 1.5 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 11.1/50.2 MB 1.5 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 11.2/50.2 MB 1.5 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 11.4/50.2 MB 1.5 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 11.5/50.2 MB 1.6 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 11.5/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 11.6/50.2 MB 1.5 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 11.7/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 11.8/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 11.9/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 11.9/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 12.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 12.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 12.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 12.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 12.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 12.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 12.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 12.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 12.7/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 12.8/50.2 MB 1.6 MB/s eta 0:00:23\n",
      "     ---------- ----------------------------- 12.9/50.2 MB 1.6 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.0/50.2 MB 1.6 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.0/50.2 MB 1.6 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.0/50.2 MB 1.6 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.0/50.2 MB 1.6 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.6 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 13.1/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 13.9/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.0/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.0/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.0/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.0/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.0/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.1/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.2/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.3/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.4/50.2 MB 1.5 MB/s eta 0:00:25\n",
      "     ----------- ---------------------------- 14.5/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 14.6/50.2 MB 1.5 MB/s eta 0:00:24\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.6 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.6 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.6 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.6 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.6 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.6 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.6 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.5 MB/s eta 0:00:23\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.5 MB/s eta 0:00:23\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.5 MB/s eta 0:00:23\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.5 MB/s eta 0:00:23\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.5 MB/s eta 0:00:23\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.5 MB/s eta 0:00:23\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.5 MB/s eta 0:00:23\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.6/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.4 MB/s eta 0:00:25\n",
      "     ------------ --------------------------- 15.7/50.2 MB 1.1 MB/s eta 0:00:32\n",
      "     ------------ --------------------------- 16.0/50.2 MB 1.1 MB/s eta 0:00:31\n",
      "     ------------ --------------------------- 16.0/50.2 MB 1.1 MB/s eta 0:00:31\n",
      "     ------------ --------------------------- 16.0/50.2 MB 1.1 MB/s eta 0:00:31\n",
      "     ------------ --------------------------- 16.0/50.2 MB 1.1 MB/s eta 0:00:31\n",
      "     ------------ --------------------------- 16.3/50.2 MB 1.1 MB/s eta 0:00:31\n",
      "     ------------- -------------------------- 16.5/50.2 MB 1.1 MB/s eta 0:00:30\n",
      "     ------------- -------------------------- 16.5/50.2 MB 1.1 MB/s eta 0:00:30\n",
      "     ------------- -------------------------- 16.5/50.2 MB 1.1 MB/s eta 0:00:30\n",
      "     ------------- -------------------------- 17.2/50.2 MB 1.2 MB/s eta 0:00:29\n",
      "     ------------- -------------------------- 17.3/50.2 MB 1.2 MB/s eta 0:00:28\n",
      "     ------------- -------------------------- 17.5/50.2 MB 1.2 MB/s eta 0:00:28\n",
      "     ------------- -------------------------- 17.5/50.2 MB 1.2 MB/s eta 0:00:28\n",
      "     ------------- -------------------------- 17.5/50.2 MB 1.2 MB/s eta 0:00:28\n",
      "     ------------- -------------------------- 17.5/50.2 MB 1.2 MB/s eta 0:00:28\n",
      "     ------------- -------------------------- 17.5/50.2 MB 1.2 MB/s eta 0:00:29\n",
      "     -------------- ------------------------- 17.6/50.2 MB 1.2 MB/s eta 0:00:28\n",
      "     -------------- ------------------------- 17.6/50.2 MB 1.2 MB/s eta 0:00:29\n",
      "     -------------- ------------------------- 17.7/50.2 MB 1.2 MB/s eta 0:00:28\n",
      "     -------------- ------------------------- 17.8/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 17.9/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.0/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.1/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.1/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.2/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.3/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.4/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.5/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.6/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.6/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.7/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     -------------- ------------------------- 18.8/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     --------------- ------------------------ 18.9/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     --------------- ------------------------ 19.0/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     --------------- ------------------------ 19.1/50.2 MB 1.2 MB/s eta 0:00:26\n",
      "     --------------- ------------------------ 19.1/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     --------------- ------------------------ 19.2/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     --------------- ------------------------ 19.3/50.2 MB 1.2 MB/s eta 0:00:27\n",
      "     --------------- ------------------------ 19.4/50.2 MB 1.2 MB/s eta 0:00:26\n",
      "     --------------- ------------------------ 19.4/50.2 MB 1.2 MB/s eta 0:00:26\n",
      "     --------------- ------------------------ 19.5/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     --------------- ------------------------ 19.7/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     --------------- ------------------------ 19.8/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     --------------- ------------------------ 19.8/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     --------------- ------------------------ 19.9/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     --------------- ------------------------ 20.0/50.2 MB 1.2 MB/s eta 0:00:26\n",
      "     ---------------- ----------------------- 20.1/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.1/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.1/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.1/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.1/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.2/50.2 MB 1.2 MB/s eta 0:00:26\n",
      "     ---------------- ----------------------- 20.3/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.4/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.5/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.6/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.7/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.8/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.8/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 20.9/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 21.0/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 21.1/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 21.2/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ---------------- ----------------------- 21.3/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 21.4/50.2 MB 1.2 MB/s eta 0:00:25\n",
      "     ----------------- ---------------------- 21.5/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 21.6/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 21.7/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 21.7/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 21.8/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 21.9/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 22.0/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 22.0/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 22.1/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 22.2/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 22.2/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ----------------- ---------------------- 22.3/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 22.4/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 22.5/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ----------------- ---------------------- 22.6/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 22.7/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 22.8/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 22.9/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.0/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.1/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.1/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.1/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.1/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.1/50.2 MB 1.2 MB/s eta 0:00:24\n",
      "     ------------------ --------------------- 23.2/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.3/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.4/50.2 MB 1.2 MB/s eta 0:00:23\n",
      "     ------------------ --------------------- 23.5/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 23.5/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 23.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 23.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 23.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 23.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 23.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------ --------------------- 23.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.2/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.3/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.3/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.3/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.3/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.3/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.4/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.5/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     ------------------- -------------------- 24.6/50.2 MB 1.2 MB/s eta 0:00:22\n",
      "     -------------------- ------------------- 25.4/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 25.5/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 25.6/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 25.6/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 25.6/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 25.6/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 25.6/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 25.6/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 25.6/50.2 MB 1.2 MB/s eta 0:00:21\n",
      "     -------------------- ------------------- 26.0/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 26.0/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 26.0/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 26.0/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 26.0/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 26.0/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 26.0/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 26.0/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 26.6/50.2 MB 1.7 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 26.8/50.2 MB 1.7 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 26.8/50.2 MB 1.7 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 27.0/50.2 MB 1.7 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 27.0/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 27.1/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 27.2/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 27.4/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 27.5/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 27.5/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 27.5/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     --------------------- ------------------ 27.5/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     ---------------------- ----------------- 27.6/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     ---------------------- ----------------- 27.7/50.2 MB 1.6 MB/s eta 0:00:15\n",
      "     ---------------------- ----------------- 27.8/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 27.9/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 27.9/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.0/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.1/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.2/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.3/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.4/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.5/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.6/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.7/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ---------------------- ----------------- 28.8/50.2 MB 1.6 MB/s eta 0:00:14\n",
      "     ----------------------- ---------------- 28.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.0/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.1/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.2/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.2/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.3/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.4/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.5/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.5/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.6/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.6/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.7/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.8/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.8/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 29.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 30.0/50.2 MB 1.5 MB/s eta 0:00:14\n",
      "     ------------------------ --------------- 30.9/50.2 MB 1.6 MB/s eta 0:00:13\n",
      "     ------------------------ --------------- 31.1/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 31.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 31.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 31.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 31.3/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 31.4/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 31.5/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 31.6/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 31.7/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 31.7/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 31.8/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 31.8/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 31.9/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.0/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.1/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 32.2/50.2 MB 1.6 MB/s eta 0:00:12\n",
      "     -------------------------- ------------- 33.0/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.1/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.1/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.2/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.4/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.4/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.5/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.6/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.7/50.2 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 33.9/50.2 MB 1.7 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.0/50.2 MB 1.7 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.0/50.2 MB 1.7 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.1/50.2 MB 1.7 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.1/50.2 MB 1.7 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.1/50.2 MB 1.7 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.1/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.2/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.3/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.3/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.4/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.5/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.5/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.6/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.7/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.8/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 34.9/50.2 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 35.0/50.2 MB 1.8 MB/s eta 0:00:09\n",
      "     --------------------------- ------------ 35.0/50.2 MB 1.8 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.1/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.2/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.4/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.5/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.6/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.7/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.7/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.7/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.7/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.7/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.8/50.2 MB 1.6 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.8/50.2 MB 1.6 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.9/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 35.9/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 36.0/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 36.1/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 36.2/50.2 MB 1.6 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 36.3/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 36.3/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 36.4/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 36.4/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 36.5/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 36.6/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 36.7/50.2 MB 1.7 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 36.8/50.2 MB 1.6 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 36.9/50.2 MB 1.6 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 37.0/50.2 MB 1.6 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 37.1/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 37.2/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 37.3/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 37.3/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 37.5/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 37.5/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 37.7/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 38.7/50.2 MB 1.7 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 38.7/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 38.7/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 38.7/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 38.8/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------- -------- 38.9/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------- -------- 38.9/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------- -------- 39.0/50.2 MB 1.6 MB/s eta 0:00:08\n",
      "     ------------------------------- -------- 39.1/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.2/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.2/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.3/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.4/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.5/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.6/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.7/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.8/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.9/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 39.9/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 40.0/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 40.1/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     -------------------------------- ------- 40.2/50.2 MB 1.6 MB/s eta 0:00:07\n",
      "     -------------------------------- ------- 40.3/50.2 MB 1.8 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 40.4/50.2 MB 1.8 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 40.5/50.2 MB 1.8 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 40.6/50.2 MB 1.8 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 40.7/50.2 MB 1.8 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 40.8/50.2 MB 1.7 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 40.9/50.2 MB 1.7 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.0/50.2 MB 1.7 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.0/50.2 MB 1.7 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.0/50.2 MB 1.7 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.0/50.2 MB 1.7 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.0/50.2 MB 1.7 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.0/50.2 MB 1.7 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.1/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.2/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 41.3/50.2 MB 1.6 MB/s eta 0:00:06\n",
      "     --------------------------------- ------ 42.3/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 42.3/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 42.3/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 42.3/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 42.3/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 42.4/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 42.5/50.2 MB 1.7 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 42.6/50.2 MB 1.7 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 42.6/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 42.7/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 42.8/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 42.8/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 42.8/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.0/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.0/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.1/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.1/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.2/50.2 MB 1.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.2/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.3/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.3/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.4/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.4/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.5/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.6/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.7/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.7/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.8/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.9/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.9/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.9/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 43.9/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 43.9/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 44.0/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 44.1/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 44.1/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 44.2/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 44.2/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 44.3/50.2 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 44.4/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.5/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.5/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.6/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.7/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.7/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.8/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 44.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 45.0/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 45.1/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.2/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.3/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.3/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.4/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.5/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.5/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.6/50.2 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.7/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.7/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.8/50.2 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 45.9/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 46.0/50.2 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ --- 46.1/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 46.2/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 46.3/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 46.3/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.4/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.5/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.6/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.6/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.6/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.6/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.7/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.8/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.9/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 46.9/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 47.0/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 47.0/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 47.1/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 47.2/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 47.3/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 47.3/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 47.4/50.2 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 47.5/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 47.5/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 47.6/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 47.6/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 47.7/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 47.8/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 47.9/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 47.9/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 47.9/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.0/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.1/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.2/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.3/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.3/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.4/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.5/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.5/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.5/50.2 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.6/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.6/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.7/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.8/50.2 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 48.9/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.0/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.1/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.1/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.3/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.4/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.5/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.5/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.6/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.6/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.6/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.7/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.8/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.8/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.9/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  50.0/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  50.1/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  50.2/50.2 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 50.2/50.2 MB 1.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\microzaib\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages (from llama-cpp-python) (4.10.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\microzaib\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\microzaib\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\microzaib\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 30.7/45.5 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 30.7/45.5 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 30.7/45.5 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.5/45.5 kB 226.3 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp39-cp39-win_amd64.whl size=3282294 sha256=17545ee4fc9ffe27f2da561f346b1e265bb62c137ba9f078abda80c5d3385352\n",
      "  Stored in directory: c:\\users\\microzaib\\appdata\\local\\pip\\cache\\wheels\\94\\e4\\60\\db177ca419dd0c984518203f4c3fd13de017fcb1012ec03272\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.78\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4913cc5e-ccea-45d7-91ad-1975a503dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28527810-3bd3-4dbd-b343-a42305600310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = qwen2-0_5b\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\" \", \" \", \"i n\", \" t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q8_0:  170 tensors\n",
      "llm_load_vocab: special tokens cache size = 293\n",
      "llm_load_vocab: token to piece cache size = 0.9338 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 896\n",
      "llm_load_print_meta: n_head           = 14\n",
      "llm_load_print_meta: n_head_kv        = 2\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 128\n",
      "llm_load_print_meta: n_embd_v_gqa     = 128\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 630.17 M\n",
      "llm_load_print_meta: model size       = 638.74 MiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = qwen2-0_5b\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 ''\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =   638.74 MiB\n",
      "...........................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     6.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    6.00 MiB, K (f16):    3.00 MiB, V (f16):    3.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   298.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 846\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'qwen2.attention.head_count': '14', 'general.name': 'qwen2-0_5b', 'general.architecture': 'qwen2', 'qwen2.block_count': '24', 'qwen2.context_length': '32768', 'qwen2.attention.head_count_kv': '2', 'qwen2.embedding_length': '896', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '4864', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '7', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Qwen2-0.5B-Instruct-GGUF\n",
    "model_path = \"./Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf\"\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=model_path, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae51f3ed-c296-4ff1-92f3-95a8346434d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     116.31 ms\n",
      "llama_print_timings:      sample time =      57.67 ms /   137 runs   (    0.42 ms per token,  2375.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.24 ms /    23 tokens (    5.05 ms per token,   197.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1512.95 ms /   136 runs   (   11.12 ms per token,    89.89 tokens per second)\n",
      "llama_print_timings:       total time =    1835.44 ms /   159 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-1545800b-5787-4eaf-b76f-c7da36ff1f2f',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1718571228,\n",
       " 'model': './Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf',\n",
       " 'choices': [{'text': '.\\n\\n```python\\nimport numpy as np\\n\\nx = np.linspace(-2, 3, 40)\\ny = x ** 3 - x ** 2 - x\\n\\nplt.plot(x, y)\\n```\\n\\nThis code uses the `numpy` library to generate a 40-element array of values from -2 to 3 for the independent variable `x`. It then computes the function `f(x) = x^3 - x^2 - x` for this set of data and plots it with the `plot()` method. The resulting plot should look like this:\\n\\n![Function Plot](https://i.imgur.com/9yD8n.jpg)',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 23, 'completion_tokens': 136, 'total_tokens': 159}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(\"Python code to plot the function y=x**3-x**2-x for x between -2 and 3\", max_tokens=1000)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a898fa79-8969-4822-b5e2-b9190a842114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 356 tensors from ./stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = stablelm\n",
      "llama_model_loader: - kv   1:                               general.name str              = stable-code-instruct-3b\n",
      "llama_model_loader: - kv   2:                    stablelm.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\n",
      "llama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\n",
      "llama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50304]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
      "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\" \", \" t\", \" a\", \"h e\", \"i n...\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:  130 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 52\n",
      "llm_load_vocab: token to piece cache size = 0.2987 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = stablelm\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50304\n",
      "llm_load_print_meta: n_merges         = 50009\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 2560\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 20\n",
      "llm_load_print_meta: n_embd_head_k    = 80\n",
      "llm_load_print_meta: n_embd_head_v    = 80\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 6912\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 2.80 B\n",
      "llm_load_print_meta: model size       = 2.77 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = stable-code-instruct-3b\n",
      "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 ''\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2833.50 MiB\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   108.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1095\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'stable-code-instruct-3b', 'stablelm.attention.layer_norm_epsilon': '0.000010', 'general.architecture': 'stablelm', 'stablelm.context_length': '16384', 'stablelm.rope.dimension_count': '20', 'stablelm.embedding_length': '2560', 'stablelm.block_count': '32', 'stablelm.feed_forward_length': '6912', 'stablelm.attention.head_count': '32', 'tokenizer.ggml.model': 'gpt2', 'stablelm.use_parallel_residual': 'true', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '0', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\\n' + system_message + '<|im_end|>\\n'}}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\n",
      "' + system_message + '<|im_end|>\n",
      "'}}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# stable_code_instruct-3B-GGUF\n",
    "model_path = \"./stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf\"\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=model_path, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393fe244-8878-4b5d-b306-e9344003e034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     401.43 ms\n",
      "llama_print_timings:      sample time =      22.31 ms /   152 runs   (    0.15 ms per token,  6812.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     401.35 ms /    27 tokens (   14.86 ms per token,    67.27 tokens per second)\n",
      "llama_print_timings:        eval time =    7892.81 ms /   151 runs   (   52.27 ms per token,    19.13 tokens per second)\n",
      "llama_print_timings:       total time =    8397.07 ms /   178 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-e3a44464-4cbd-4c88-be4b-de6b43551f76',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1718577908,\n",
       " 'model': './stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf',\n",
       " 'choices': [{'text': \".0:\\n\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Define the function\\ndef f(x):\\n    return (x ** 3) - (x ** 2)) - x\\n\\n# Create an array of x values between -2 and 3.0\\nx = np.linspace(-2, 3.0), 100)\\n\\n# Evaluate the function at each x value\\ny = f(x)\\n\\n# Plot the function\\nplt.plot(x, y)\\nplt.title('Function y = x^3 - x^2 - x')\\nplt.grid(True)\\nplt.show()\\n```<|im_end|>\\n\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 27, 'completion_tokens': 151, 'total_tokens': 178}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(\"Python code to plot the function y=x**3-x**2-x for x between -2 and 3\", max_tokens=1000)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44059c1-123b-4a17-89aa-95674a1c256c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     401.43 ms\n",
      "llama_print_timings:      sample time =      23.90 ms /   172 runs   (    0.14 ms per token,  7196.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.85 ms /    12 tokens (   20.82 ms per token,    48.03 tokens per second)\n",
      "llama_print_timings:        eval time =    8872.78 ms /   171 runs   (   51.89 ms per token,    19.27 tokens per second)\n",
      "llama_print_timings:       total time =    9227.73 ms /   183 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-b92a29fb-2893-47b9-829c-a3342634a901',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1718577989,\n",
       " 'model': './stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf',\n",
       " 'choices': [{'text': ':\\n\\n```python\\ndef fibonacci(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    else:\\n        fib = [0, 1]\\n        for i in range(2, n):\\n            fib.append(fib[i-1] + fib[i-2]))\\n        return fib\\n\\n\\nprint(fibonacci(15)))\\n```\\n\\nThis code defines a function `fibonacci` that takes an integer `n` as input and returns the first `n` numbers in the Fibonacci sequence. The code then calls this function with `n=15` to print the first 15 numbers in the Fibonacci sequence, and uses the `print` function to display these numbers.<|im_end|>\\n',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 16, 'completion_tokens': 171, 'total_tokens': 187}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(\"Python code to generate fibonnaci sequence, print first 15 numbers\", max_tokens=1000)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1091faba-039c-443c-aa26-ab99102b3a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 164 tensors from ./gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:   37 tensors\n",
      "llama_model_loader: - type q8_0:  127 tensors\n",
      "llm_load_vocab: special tokens cache size = 388\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_tensors: ggml ctx size =    0.08 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2539.93 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 601\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'gemma-2b-it', 'general.architecture': 'gemma', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.attention.head_count_kv': '1', 'gemma.embedding_length': '2048', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'gemma.attention.key_length': '256', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '3'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# gemma-2b-it-GGUF\n",
    "model_path = \"./gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf\"\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=model_path, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dcdb1f3-c2ff-42da-9a96-078cd0b71b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     386.52 ms\n",
      "llama_print_timings:      sample time =      73.37 ms /   109 runs   (    0.67 ms per token,  1485.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     386.48 ms /    26 tokens (   14.86 ms per token,    67.27 tokens per second)\n",
      "llama_print_timings:        eval time =    5134.02 ms /   108 runs   (   47.54 ms per token,    21.04 tokens per second)\n",
      "llama_print_timings:       total time =    5754.03 ms /   134 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-206da09f-42ad-4435-92dc-f1a60d55aef8',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1718578383,\n",
       " 'model': './gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf',\n",
       " 'choices': [{'text': '\\n\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Define the function\\ndef f(x):\\n  return x ** 3 - x ** 2 - x\\n\\n# Create a numpy array of x values\\nx = np.linspace(-2, 3, 100)\\n\\n# Evaluate the function for each x value\\ny = f(x)\\n\\n# Plot the function\\nplt.plot(x, y)\\n\\n# Show the plot\\nplt.show()\\n```',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 26, 'completion_tokens': 108, 'total_tokens': 134}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(\"Python code to plot the function y=x**3-x**2-x for x between -2 and 3\", max_tokens=1000)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beee4283-2359-45ed-af40-79417c90c3b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     386.52 ms\n",
      "llama_print_timings:      sample time =      74.77 ms /   111 runs   (    0.67 ms per token,  1484.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     195.53 ms /     6 tokens (   32.59 ms per token,    30.69 tokens per second)\n",
      "llama_print_timings:        eval time =    5153.04 ms /   110 runs   (   46.85 ms per token,    21.35 tokens per second)\n",
      "llama_print_timings:       total time =    5588.54 ms /   116 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-796e9e23-d9ce-44b5-8f0c-612d82ab1dde',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1718578452,\n",
       " 'model': './gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf',\n",
       " 'choices': [{'text': '\\n\\nAn Large Language Model (LLM) is a powerful artificial intelligence model that has surpassed human capabilities in language understanding and generation. LLMs are trained on massive datasets of text and code, allowing them to perform a wide range of natural language processing (NLP) tasks, including:\\n\\n- Text generation\\n- Language translation\\n- Question answering\\n- Summarization\\n- Sentiment analysis\\n\\nLLMs are typically used in research and development, education, and commercial applications. Some well-known LLMs include GPT-3, BERT, and T5.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 7, 'completion_tokens': 110, 'total_tokens': 117}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(\"What is an LLM?\", max_tokens=1000)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "292891f1-fa49-47b0-803c-d3c0bcaeef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = qwen2-0_5b\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\" \", \" \", \"i n\", \" t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q8_0:  170 tensors\n",
      "llm_load_vocab: special tokens cache size = 293\n",
      "llm_load_vocab: token to piece cache size = 0.9338 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 896\n",
      "llm_load_print_meta: n_head           = 14\n",
      "llm_load_print_meta: n_head_kv        = 2\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 128\n",
      "llm_load_print_meta: n_embd_v_gqa     = 128\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 630.17 M\n",
      "llm_load_print_meta: model size       = 638.74 MiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = qwen2-0_5b\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 ''\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =   638.74 MiB\n",
      "...........................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     6.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    6.00 MiB, K (f16):    3.00 MiB, V (f16):    3.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   298.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 846\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'qwen2.attention.head_count': '14', 'general.name': 'qwen2-0_5b', 'general.architecture': 'qwen2', 'qwen2.block_count': '24', 'qwen2.context_length': '32768', 'qwen2.attention.head_count_kv': '2', 'qwen2.embedding_length': '896', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '4864', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '7', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n",
      "\n",
      "llama_print_timings:        load time =     231.53 ms\n",
      "llama_print_timings:      sample time =      12.68 ms /    32 runs   (    0.40 ms per token,  2524.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     231.49 ms /     7 tokens (   33.07 ms per token,    30.24 tokens per second)\n",
      "llama_print_timings:        eval time =     345.17 ms /    31 runs   (   11.13 ms per token,    89.81 tokens per second)\n",
      "llama_print_timings:       total time =     625.09 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-bae5d146-effc-488c-a223-bafd3021a995', 'object': 'text_completion', 'created': 1718580791, 'model': './Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf', 'choices': [{'text': ' The Law School Admission Test (LSAT) is a standardized test that assesses the ability of applicants to learn, understand and apply the law. The LSAT', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 7, 'completion_tokens': 32, 'total_tokens': 39}}\n"
     ]
    }
   ],
   "source": [
    "def generate_gguf_response(prompt, max_tokens, model_path = \"./Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf\" ):\n",
    "    from llama_cpp import Llama\n",
    "    llm = Llama(model_path=model_path, streaming=True)\n",
    "    response = llm(prompt, max_tokens=max_tokens)\n",
    "    print(response)\n",
    "    yield response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54e6c7be-1b1f-4d37-a22d-87c6e760b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://localhost:1238\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:1238/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.8.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = qwen2-0_5b\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\" \", \" \", \"i n\", \" t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q8_0:  170 tensors\n",
      "llm_load_vocab: special tokens cache size = 293\n",
      "llm_load_vocab: token to piece cache size = 0.9338 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 896\n",
      "llm_load_print_meta: n_head           = 14\n",
      "llm_load_print_meta: n_head_kv        = 2\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 128\n",
      "llm_load_print_meta: n_embd_v_gqa     = 128\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 630.17 M\n",
      "llm_load_print_meta: model size       = 638.74 MiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = qwen2-0_5b\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 ''\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =   638.74 MiB\n",
      "...........................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     6.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    6.00 MiB, K (f16):    3.00 MiB, V (f16):    3.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   298.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 846\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'qwen2.attention.head_count': '14', 'general.name': 'qwen2-0_5b', 'general.architecture': 'qwen2', 'qwen2.block_count': '24', 'qwen2.context_length': '32768', 'qwen2.attention.head_count_kv': '2', 'qwen2.embedding_length': '896', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '4864', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '7', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n",
      "\n",
      "llama_print_timings:        load time =     153.56 ms\n",
      "llama_print_timings:      sample time =      13.30 ms /    32 runs   (    0.42 ms per token,  2406.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.46 ms /    23 tokens (    6.67 ms per token,   149.88 tokens per second)\n",
      "llama_print_timings:        eval time =     341.87 ms /    31 runs   (   11.03 ms per token,    90.68 tokens per second)\n",
      "llama_print_timings:       total time =     542.88 ms /    54 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-fe3612d0-3d2a-46df-b35e-f4a6493b2a6e', 'object': 'text_completion', 'created': 1718580816, 'model': './Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf', 'choices': [{'text': ', using a custom plotting library that supports multiple output formats.\\n\\nNote: This problem requires understanding of the mathematical concept of exponentiation (base raised to an exponent)', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 23, 'completion_tokens': 32, 'total_tokens': 55}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = qwen2-0_5b\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\" \", \" \", \"i n\", \" t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q8_0:  170 tensors\n",
      "llm_load_vocab: special tokens cache size = 293\n",
      "llm_load_vocab: token to piece cache size = 0.9338 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 896\n",
      "llm_load_print_meta: n_head           = 14\n",
      "llm_load_print_meta: n_head_kv        = 2\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 128\n",
      "llm_load_print_meta: n_embd_v_gqa     = 128\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 630.17 M\n",
      "llm_load_print_meta: model size       = 638.74 MiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = qwen2-0_5b\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 ''\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =   638.74 MiB\n",
      "...........................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     6.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    6.00 MiB, K (f16):    3.00 MiB, V (f16):    3.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   298.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 846\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'qwen2.attention.head_count': '14', 'general.name': 'qwen2-0_5b', 'general.architecture': 'qwen2', 'qwen2.block_count': '24', 'qwen2.context_length': '32768', 'qwen2.attention.head_count_kv': '2', 'qwen2.embedding_length': '896', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '4864', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '7', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n",
      "\n",
      "llama_print_timings:        load time =     158.49 ms\n",
      "llama_print_timings:      sample time =     206.09 ms /   489 runs   (    0.42 ms per token,  2372.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.45 ms /    23 tokens (    6.89 ms per token,   145.15 tokens per second)\n",
      "llama_print_timings:        eval time =    5512.55 ms /   488 runs   (   11.30 ms per token,    88.53 tokens per second)\n",
      "llama_print_timings:       total time =    6651.05 ms /   511 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-19c70e0c-eb87-4d9c-a978-d9866967197d', 'object': 'text_completion', 'created': 1718580833, 'model': './Qwen2-0.5B-Instruct-GGUF/qwen2-0_5b-instruct-q8_0.gguf', 'choices': [{'text': \".\\n\\nCertainly! Below is a Python code snippet that uses the `matplotlib` library to plot the function `y = x^3 - x^2 - x` from `-2` to `3`.\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\n# Define the equation\\ndef f(x):\\n    return x**3 - x**2 - x\\n\\n# Plot the function in a graph\\nplt.plot([-2, 3], [f(-2), f(3)], color='blue', linestyle='-')\\nplt.title('Plot of y = x^3 - x^2 - x')\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.grid(True)\\nplt.show()\\n```\\n\\n### Explanation:\\n1. **Importing the necessary libraries**: The first line imports `matplotlib.pyplot` and `mathplotlib.pyplot` from `matplotlib` as part of a package, ensuring compatibility with Python 3.\\n2. **Declaring the function**: Defines a function named `f(x)` that calculates the value of y = x^3 - x^2 - x for given x values in the range [-2, 3].\\n3. **Plotting the function**: Uses `plt.plot` to plot two curves: one from `[-2, 3]` and another from `[0, f(3)]`. The first curve represents y = x^3, while the second curve represents y = -x + 1.\\n4. **Adding a title and labels**: Sets the title of the graph as 'Plot of y = x^3 - x^2 - x' and the x-axis label to `x` from `-2` to `3`.\\n5. **Adding a grid**: Enables plotting with a grid by setting the `grid` parameter to `True`.\\n\\n### Output:\\n![Plot of y = x^3-x^2-x](https://i.imgur.com/7G0V9Jf.png)\\n\\nThis plot shows the graph of the function `y = x^3 - x^2 - x` from `-2` to `3`. The curve plotted is a quadratic function, where y increases linearly between the minimum and maximum values. The graph has been annotated with labels for clarity, and it includes a title explaining the plot's purpose. This code is designed to be used as part of a larger program, like plotting multiple graphs or visualizing data, since it integrates both matplotlib\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 23, 'completion_tokens': 489, 'total_tokens': 512}}\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=generate_gguf_response,\n",
    "    inputs=[\"text\", gr.Slider(minimum=32, maximum=1024, value=32, step = 32)],\n",
    "    outputs=[\"text\"],\n",
    "    title=\"Qwen-2 Chatbot on Ryzen AI GGUF\",\n",
    "    description=\"Qwen-2-0.5B-gguf Chatbot on Ryzen AI Laptop\",\n",
    "    concurrency_limit=4\n",
    "    #).queue(concurrency_count=2).launch(server_name=\"localhost\", server_port=1234)\n",
    "    ).queue().launch(server_name=\"localhost\", server_port=1238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6daa2bc-37ed-445e-8429-fa7236479776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 356 tensors from ./stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = stablelm\n",
      "llama_model_loader: - kv   1:                               general.name str              = stable-code-instruct-3b\n",
      "llama_model_loader: - kv   2:                    stablelm.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\n",
      "llama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\n",
      "llama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50304]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
      "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\" \", \" t\", \" a\", \"h e\", \"i n...\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:  130 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 52\n",
      "llm_load_vocab: token to piece cache size = 0.2987 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = stablelm\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50304\n",
      "llm_load_print_meta: n_merges         = 50009\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 2560\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 20\n",
      "llm_load_print_meta: n_embd_head_k    = 80\n",
      "llm_load_print_meta: n_embd_head_v    = 80\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 6912\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 2.80 B\n",
      "llm_load_print_meta: model size       = 2.77 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = stable-code-instruct-3b\n",
      "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 ''\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2833.50 MiB\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   108.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1095\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'stable-code-instruct-3b', 'stablelm.attention.layer_norm_epsilon': '0.000010', 'general.architecture': 'stablelm', 'stablelm.context_length': '16384', 'stablelm.rope.dimension_count': '20', 'stablelm.embedding_length': '2560', 'stablelm.block_count': '32', 'stablelm.feed_forward_length': '6912', 'stablelm.attention.head_count': '32', 'tokenizer.ggml.model': 'gpt2', 'stablelm.use_parallel_residual': 'true', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '0', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\\n' + system_message + '<|im_end|>\\n'}}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\n",
      "' + system_message + '<|im_end|>\n",
      "'}}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <|endoftext|>\n",
      "\n",
      "llama_print_timings:        load time =     271.83 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7042.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     271.80 ms /    14 tokens (   19.41 ms per token,    51.51 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     272.20 ms /    15 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-0e7c7a27-f451-4846-987a-163fd77fb7b7', 'object': 'text_completion', 'created': 1718723266, 'model': './stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 1, 'total_tokens': 15}}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path = \"./stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf\"\n",
    "llm = Llama(\n",
    "      model_path = model_path,\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n",
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbbe069-5774-43c3-a4b1-e00ef6d483ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 356 tensors from ./stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = stablelm\n",
      "llama_model_loader: - kv   1:                               general.name str              = stable-code-instruct-3b\n",
      "llama_model_loader: - kv   2:                    stablelm.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\n",
      "llama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\n",
      "llama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50304]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
      "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\" \", \" t\", \" a\", \"h e\", \"i n...\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:  130 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 52\n",
      "llm_load_vocab: token to piece cache size = 0.2987 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = stablelm\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50304\n",
      "llm_load_print_meta: n_merges         = 50009\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 2560\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 20\n",
      "llm_load_print_meta: n_embd_head_k    = 80\n",
      "llm_load_print_meta: n_embd_head_v    = 80\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 6912\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 2.80 B\n",
      "llm_load_print_meta: model size       = 2.77 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = stable-code-instruct-3b\n",
      "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 ''\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2833.50 MiB\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   108.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1095\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'stable-code-instruct-3b', 'stablelm.attention.layer_norm_epsilon': '0.000010', 'general.architecture': 'stablelm', 'stablelm.context_length': '16384', 'stablelm.rope.dimension_count': '20', 'stablelm.embedding_length': '2560', 'stablelm.block_count': '32', 'stablelm.feed_forward_length': '6912', 'stablelm.attention.head_count': '32', 'tokenizer.ggml.model': 'gpt2', 'stablelm.use_parallel_residual': 'true', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '0', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\\n' + system_message + '<|im_end|>\\n'}}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "\n",
      "llama_print_timings:        load time =     673.95 ms\n",
      "llama_print_timings:      sample time =      28.55 ms /   192 runs   (    0.15 ms per token,  6725.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     673.90 ms /    56 tokens (   12.03 ms per token,    83.10 tokens per second)\n",
      "llama_print_timings:        eval time =   10133.25 ms /   191 runs   (   53.05 ms per token,    18.85 tokens per second)\n",
      "llama_print_timings:       total time =   10935.78 ms /   247 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-d4f7c737-1e6b-4134-acd3-ee3866e089b2',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1718796116,\n",
       " 'model': './stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"\\nHere's the Python code using matplotlib library:\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n# Define the function\\ndef f(x):\\n return x**3 - x**2 - x\\n# Generate data for the plot\\nx = np.linspace(-2, 3), 100)\\ny = f(x)\\n# Plot the graph\\nplt.plot(x, y)\\nplt.title('Plot of y=x^3-x^2-x')\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.grid()\\nplt.show()\\n```\\nThis code generates a plot of the function y = x^3 - x^2 - x for x between -2 and 3. The plot includes gridlines, a title, and labels for the x and y axes.<|im_end|>\\n\"},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 56, 'completion_tokens': 191, 'total_tokens': 247}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "      model_path=\"./stable-code-instruct-3B-GGUF/stable-code-instruct-3b.Q8_0.gguf\",\n",
    "      chat_format=\"llama-2\"\n",
    ")\n",
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who can write perfect Python code.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Python code to plot the function y=x**3-x**2-x for x between -2 and 3.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20162f8a-0f2a-4990-89a4-96c3eb817f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa658a-b234-4ac6-9bb8-b2d7c15915ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_output_tokens, model_id, device = \"cpu\"):\n",
    "    \n",
    "    model_path=model_path_fp32(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model_name = model_id.rsplit('/')[-1] \n",
    "    model_path = model_path_fp32(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful python coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    s = time.perf_counter()\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=max_output_tokens, use_cache=True, do_sample=False)\n",
    "    e = time.perf_counter() - s\n",
    "    outputs_tkn_len = generated_ids.shape[1]\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(response)\n",
    "    yield response, \"tkn/sec: \" + str(outputs_tkn_len/e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a282d4-9754-4af4-b6d2-e5d07618364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=[\"text\", gr.Slider(minimum=32, maximum=1024, value=32, step = 32), \"text\"],\n",
    "    outputs=[\"text\", \"text\"],\n",
    "    title=\"Qwen-2 Chatbot on Ryzen AI without NPU\",\n",
    "    description=\"Qwen-2 fp-32 Chatbot on Ryzen AI Laptop\",\n",
    "    concurrency_limit=4\n",
    "    #).queue(concurrency_count=2).launch(server_name=\"localhost\", server_port=1234)\n",
    "    ).queue().launch(server_name=\"localhost\", server_port=1236)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "176993d9-942c-4636-9766-d66d2df66a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up some sample data points to plot\n",
    "x = np.linspace(-5, 5, 40)\n",
    "y = x**3 - x**2 - x\n",
    "\n",
    "# Plot the function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Function: f(x) = x^3 - x^2 - x')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405b869-538f-4836-94d3-15b68ba167fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.system(\"optimum-cli export onnx --help\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02520c3-5c1d-42cf-9f7d-cf382aff2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run([\"optimum-cli\", \"export\", \"onnx\", \"--help\"], capture_output=True)\n",
    "print(result.stdout.decode())  # Print the captured output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c9764-0c44-4db5-8f93-14a6c1eca2ac",
   "metadata": {},
   "source": [
    "- Create activation channel scales for the downloaded modelfor smooth quantization\n",
    "- To be incorporated later from https://github.com/mit-han-lab/smoothquant\n",
    "- Using downloaded act_scales in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794a123-07f5-4e2a-bcd1-b69bc1cd00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantize the downloaded fp32 model to int8 quantized model after first smooth-quantizing\n",
    "def model_quant(model_path_fp32, model_id):\n",
    "    \"\"\" Quantizes and saves the downloaded fp32 pretratined model to the specified dir, first as smooth quant and then as int8 quantized model\n",
    "    Args: \n",
    "    model_path_fp32 - downloaded fp32 model path  \n",
    "    model_path_pt - pretrained model path (e.g. facebook/opt-1.3b)\n",
    "    Output: smooth quant model saved in specified dir \"\"\"\n",
    "    \n",
    "    path = model_path_fp32(model_id)\n",
    "    model_name = model_id.rsplit('/')[-1]\n",
    "    if not os.path.exists(path):\n",
    "            print(f\"Pretrained fp32 model not found, exiting..\")\n",
    "            exit(1)\n",
    "    model = OPTForCausalLM.from_pretrained(path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model.tokenizer = tokenizer \n",
    "     # get actuvation scales for smooth quantization\n",
    "    model_act = \"%s.pt\"%model_name\n",
    "    act_scales = torch.load(\"./ext/smoothquant/act_scales/\" + model_act)\n",
    "    # smooth quantize\n",
    "    smooth_lm(model, act_scales, 0.5)\n",
    "    print(model)\n",
    "    # initialize with random prompt\n",
    "    prompt = ''.join(random.choices(string.ascii_lowercase + \" \", k=model.config.max_position_embeddings))\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\")  # takes a lot of time\n",
    "    inputs = tokenizer(\"What is meaning of life\", return_tensors=\"pt\") \n",
    "    print(f\"inputs: {inputs}\")\n",
    "    print(f\"inputs.input_ids: {inputs.input_ids}\")\n",
    "    for key in inputs.keys():\n",
    "        print(inputs[key].shape)\n",
    "        print(inputs[key])\n",
    "    model_out = model(inputs.input_ids)\n",
    "    print(f\"{(model_out.logits.shape)=}\")\n",
    "    out_dir = \"./%s_smoothquant\"%model_name\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    model.save_pretrained(out_dir+\"/model_onnx\")\n",
    "    print(f\"Saving Smooth Quant fp32 model...\\n \")\n",
    "\n",
    "    print(f\"Quantizing model with Optimum...\\n \")\n",
    "    #proc = subprocess.Popen('cmd.exe', stdin = subprocess.PIPE, stdout = subprocess.PIPE)\n",
    "    os.system('optimum-cli export onnx -m out_dir\\model_onnx --task text-generation-with-past out_dir\\model_onnx_int8  --framework pt --no-post-process')\n",
    "    print(f\"Saving quantized int8 model ...\\n \")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c8936-a152-4645-8275-18fea3268db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant(model_path_fp32, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc992ca0-a9c5-4c0d-8e37-0b3e634f174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = model_path_fp32(model_id)\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Pretrained fp32 model not found, exiting..\")\n",
    "    exit(1)\n",
    "model = OPTForCausalLM.from_pretrained(path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_pt)\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ca16e-5554-4d04-be57-f72ff90da90e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = model_id.rsplit('/')[-1]\n",
    "model_act = \"%s.pt\"%model_name\n",
    "act_scales = torch.load(\"./ext/smoothquant/act_scales/\" + model_act)\n",
    "# act_scales = torch.load(os.getenv(\"PYTORCH_AIE_PATH\") + \"/ext/smoothquant/act_scales/\" + \"opt-1.3b.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547cb8ab-26bc-41ef-820b-3571c0c26c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_lm(model, act_scales, 0.5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ec4de-118f-4d36-b3b5-893a98bb7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''.join(random.choices(string.ascii_lowercase + \" \", k=model.config.max_position_embeddings))\n",
    "#inputs = tokenizer(prompt, return_tensors=\"pt\")  # takes a lot of time\n",
    "inputs = tokenizer(\"What is meaning of life\", return_tensors=\"pt\") \n",
    "print(f\"inputs: {inputs}\")\n",
    "print(f\"inputs.input_ids: {inputs.input_ids}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a4fb7-4769-4a56-a6dd-7bed2a6d0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inputs.keys():\n",
    "    print(inputs[key].shape)\n",
    "    print(inputs[key])\n",
    "model_out = model(inputs.input_ids)\n",
    "print(f\"{(model_out.logits.shape)=}\")\n",
    "out_dir = \"./%s_smoothquant\"%model_name\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "model.save_pretrained(out_dir+\"/model_onnx\")\n",
    "print(f\"Saving Smooth Quant fp32 model...\\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d428bac-0484-4046-a313-952b0fa6555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Quantizing model with Optimum...\\n \")\n",
    "#proc = subprocess.Popen('cmd.exe', stdin = subprocess.PIPE, stdout = subprocess.PIPE)\n",
    "os.system('optimum-cli export onnx -m out_dir\\model_onnx --task text-generation-with-past out_dir\\model_onnx_int8  --framework pt --no-post-process')\n",
    "print(f\"Saving quantized int8 model ...\\n \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093108cb-5f5d-45b2-b13e-c1987a117fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path to vaip_config.json file needed to run Vitis AI Execution Provider and quantized model\n",
    "model_id = \"Qwen/Qwen2-0.5B-Instruct\" #path to pretrained model on huggingface hub\n",
    "model_name = model_id.rsplit('/')[-1]\n",
    "out_dir = \"./Qwen2-0.5B-Instruct-GGUF\"\n",
    "config_file_path = \"./vaip_config.json\"\n",
    "model_path = out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e080748-ed5d-4d49-bdf6-74a248c9723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = [\"VitisAIExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "provider_options = [{'config_file': str(config_file_path)}, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa29397-656c-48af-a4da-7413537eefd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cbe8a-89d4-4860-ab97-3677b6b68462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601e306-3d1b-4a4e-b210-373b7432243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bebff5-3122-4798-ab91-3e549ad859c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092241b7-f279-49ab-82fd-d5025d0afa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who is Scarlett O'hara\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0e9a3-c294-448e-b66e-7a44fa566976",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f538d42-64e3-43e3-828d-d487b88b7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ids = model.generate(inputs.input_ids, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986af8fd-eb98-49d9-ac6f-356d8875f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3a23a-654b-4391-a169-775dba1aa527",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be7624-2ec6-4064-95a4-c4b5b05e402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"./vaip_config.json\"\n",
    "providers = [\"VitisAIExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "provider_options = [{'config_file': str(config_file_path)}, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911431c-e9aa-4c41-bd0c-fbc288fdba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(Input_text, max_output_token, model_id = \"facebook/opt-1.3b\", p_type = 0, p_options = 0):\n",
    "    model_name = model_id.rsplit('/')[-1]\n",
    "    out_dir = \"./%s_smoothquant\"%model_name    \n",
    "    onnx_model_path = out_dir+\"/model_onnx_int8\"\n",
    "    model = ORTModelForCausalLM.from_pretrained(onnx_model_path, provider = providers[p_type], provider_options = provider_options[p_options])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    inputs = tokenizer(Input_text, return_tensors=\"pt\") \n",
    "    s = time.perf_counter()\n",
    "    outputs_tkn = model.generate(inputs.input_ids, max_length=max_output_token, use_cache=True, do_sample=False)\n",
    "    e = time.perf_counter() - s\n",
    "    outputs_tkn_len = outputs_tkn.shape[1]\n",
    "    outputs = tokenizer.batch_decode(outputs_tkn,\n",
    "                                    skip_special_tokens=True, \n",
    "                                    clean_up_tokenization_spaces=False)[0]\n",
    "    print(outputs)\n",
    "    yield outputs, \"tkn/sec: \" + str(outputs_tkn_len/e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced91135-8d67-498f-8a41-e53cafe0b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(Input_text, max_output_token):\n",
    "#     inputs = tokenizer(Input_text, return_tensors=\"pt\") \n",
    "#     s = time.perf_counter()\n",
    "#     outputs_tkn = model.generate(inputs.input_ids, max_length=max_output_token, use_cache=True, do_sample=False)\n",
    "#     e = time.perf_counter() - s\n",
    "#     outputs_tkn_len = outputs_tkn.shape[1]\n",
    "#     outputs = tokenizer.batch_decode(outputs_tkn,\n",
    "#                                     skip_special_tokens=True, \n",
    "#                                     clean_up_tokenization_spaces=False)[0]\n",
    "#     print(outputs)\n",
    "#     yield outputs, \"tkn/sec: \" + str(outputs_tkn_len/e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf916390-d27f-4c71-a795-1d2cdb05015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(\n",
    "    fn=generate_text,\n",
    "    inputs=[\"text\", gr.Slider(minimum=32, maximum=256, value=32, step = 32)],\n",
    "    outputs=[\"text\", \"text\"],\n",
    "    title=\"OPT Chatbot on Ryzen AI\",\n",
    "    description=\"Simple Chatbot on Ryzen AI Laptop\",\n",
    "    concurrency_limit=4\n",
    "    #).queue(concurrency_count=2).launch(server_name=\"localhost\", server_port=1234)\n",
    "    ).queue().launch(server_name=\"localhost\", server_port=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b168e3-54bb-47c2-b06c-f7296b9adc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 164 tensors from ./gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:   37 tensors\n",
      "llama_model_loader: - type q8_0:  127 tensors\n",
      "llm_load_vocab: special tokens cache size = 388\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_tensors: ggml ctx size =    0.08 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2539.93 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 601\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'gemma-2b-it', 'general.architecture': 'gemma', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.attention.head_count_kv': '1', 'gemma.embedding_length': '2048', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'gemma.attention.key_length': '256', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '3'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m\n\u001b[0;32m      4\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_chat_completion(\n\u001b[0;32m      5\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant who provides perfect answers.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m completion:\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m         new_message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'content'"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path = \"./gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf\"\n",
    "client = Llama(model_path=model_path)\n",
    "completion = client.create_chat_completion(\n",
    "    messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant who provides perfect answers.\"},\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Where is India?\"\n",
    "      }\n",
    "    ],\n",
    "    temperature = 0.7,\n",
    "    stream = True\n",
    "    )\n",
    "\n",
    "for chunk in completion:\n",
    "     \n",
    "    if chunk['choices'][0]['delta']['content']:\n",
    "        print(chunk['choices'][0]['delta']['content'], end=\"\", flush=True)\n",
    "        new_message[\"content\"] += chunk['choices'][0]['delta']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e460dee9-c256-42bf-b513-5d0d09aff3a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(completion['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc814df5-a222-443a-b016-174737ea5553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 164 tensors from ./gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:   37 tensors\n",
      "llama_model_loader: - type q8_0:  127 tensors\n",
      "llm_load_vocab: special tokens cache size = 388\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_tensors: ggml ctx size =    0.08 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2539.93 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 601\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'gemma-2b-it', 'general.architecture': 'gemma', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.attention.head_count_kv': '1', 'gemma.embedding_length': '2048', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'gemma.attention.key_length': '256', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '3'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =     732.05 ms\n",
      "llama_print_timings:      sample time =      48.59 ms /    77 runs   (    0.63 ms per token,  1584.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     732.00 ms /    33 tokens (   22.18 ms per token,    45.08 tokens per second)\n",
      "llama_print_timings:        eval time =    3668.96 ms /    76 runs   (   48.28 ms per token,    20.71 tokens per second)\n",
      "llama_print_timings:       total time =    4564.85 ms /   109 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = np.linspace(-5, 5, 100)\n",
      "y = x**2\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.xlabel('X')\n",
      "plt.ylabel('Y')\n",
      "plt.title('Parabola')\n",
      "plt.show()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"./gemma-2b-it-GGUF/gemma-2b-it-q8_0.gguf\"\n",
    "client = Llama(model_path=model_path)\n",
    "completion = client.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who provides perfect answers.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Python code to plot a parabola?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Initialize an empty string to store the assistant's response\n",
    "assistant_response = \"\"\n",
    "\n",
    "for chunk in completion:\n",
    "    if \"content\" in chunk['choices'][0]['delta']:\n",
    "        assistant_response += chunk['choices'][0]['delta']['content']\n",
    "\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "807e445b-d142-42f9-9fce-fcaddc83906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define the coefficients of the quadratic equation\n",
    "a = 1\n",
    "b = -5\n",
    "c = 6\n",
    "\n",
    "# calculate the x and y coordinates of the vertex of the parabola\n",
    "x_vertex = -b/(2*a)\n",
    "y_vertex = a*x_vertex**2 + b*x_vertex + c\n",
    "\n",
    "# plot the parabola\n",
    "plt.plot(x_vertex, y_vertex)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f0cfff-a9dc-4558-9f78-a79914d2d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0421982-8900-420f-bbbe-0708ab7f00e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a361606e12f14a249a78359df6de3d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf:  71%|#######1  | 3.51G/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MicroZaib\\.cache\\huggingface\\hub\\models--NousResearch--Hermes-2-Pro-Llama-3-8B-GGUF. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n",
    "    filename=\"*Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb55656-cef5-4b00-a138-c17ac0a463aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryzenai-transformers",
   "language": "python",
   "name": "ryzenai-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
