{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd57e3-c36e-49ad-b12d-b7525aee1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using phi2.gguf model\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/phi-2-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"your are a python coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"code to plot the function y=x**3-x**2-x.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940dd3d-1873-4f8b-8b15-bc30d8e27ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using stable-code-instruct.gguf model\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"brittlewis12/stable-code-instruct-3b-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"your are a python coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"code to generate fibonacci sequence. print first 15 numbers.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab59338-a078-40c0-9b92-2618a524f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Qwen2-0.5B.gguf model\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"your are a python coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"code to print Hello World ten times.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de5ff0-5d8d-4177-afab-d78b4c0b26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using meta-llama-8B-instruct.gguf model\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"lmstudio-ai/gemma-2b-it-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Python coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Python code to plot a sin function\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    " stream = True\n",
    "    \n",
    ")\n",
    "for chunk in completion:\n",
    "    streamed_response = chunk.choices[0].delta.content\n",
    "    print(streamed_response, end=\"\", flush=True)\n",
    "\n",
    "# llm_response = completion.choices[0].message.content \n",
    "# if llm_response.startswith(\"```python\"):\n",
    "#     formatted_code = llm_response\n",
    "# else:\n",
    "#     # Add \"python\" prefix\n",
    "#     formatted_code = f\"```python\\n{llm_response}\\n```\"\n",
    "    \n",
    "#     # Extract the code snippet using regular expressions\n",
    "#     code_match = re.search(r\"```python\\n(.*?)\\n```\", formatted_code, re.DOTALL)\n",
    "# if code_match:\n",
    "#     code_snippet = code_match.group(1)\n",
    "#     print(\"Chatbot:\", code_snippet)\n",
    "# else:\n",
    "#     print(\"Chatbot\", \"I apologize, but I couldn't find a valid code snippet in the response.\")\n",
    "\n",
    " \n",
    "# print(completion.choices[0].message)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c367835-672b-4989-b5b3-a4048c71f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"brittlewis12/stable-code-instruct-3b-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Python coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Python code to plot a sin function\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    " stream = True\n",
    "    \n",
    ")\n",
    "for chunk in completion:\n",
    "    streamed_response = chunk.choices[0].delta.content\n",
    "    print(streamed_response, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf2c44-8a8e-4f56-9be3-3ff3fb8fa317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an array of x values from 0 to 2*pi (inclusive) with 100 points\n",
    "x = np.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Calculate the sine of each value in x\n",
    "y = np.sin(x)\n",
    "\n",
    "# Create a plot using matplotlib\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Sine Function')\n",
    "plt.xlabel('X axis')\n",
    "plt.ylabel('Y axis')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a267be-79f2-4699-802c-d584fb0f8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Python coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Code to plot a parabola.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "    stream = True    \n",
    ")\n",
    "for chunk in completion:\n",
    "    streamed_response = chunk.choices[0].delta.content\n",
    "    print(streamed_response, end=\"\", flush=True)\n",
    "\n",
    "# print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e419e2-88e6-487f-b33b-685580a16307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameters of the parabola\n",
    "a = 1  # coefficient of x^2\n",
    "b = -2  # coefficient of x\n",
    "c = 3  # constant term\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Calculate y values using the equation of the parabola\n",
    "y = a * x**2 + b * x + c\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Parabola')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05cd718-505a-4396-acf7-f3727ba41c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation with meta-llama-8B-instruct.gguf model\n",
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant who specializes in python coding. You always provide the most accurate code in response to queries.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, python code to create a chatbot to communicate with an LLM using Tkinter GUI.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "        messages=history,\n",
    "        temperature=0.3,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    \n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "    history.append(new_message)\n",
    "    \n",
    "    # Uncomment to see chat history\n",
    "    # import json\n",
    "    # gray_color = \"\\033[90m\"\n",
    "    # reset_color = \"\\033[0m\"\n",
    "    # print(f\"{gray_color}\\n{'-'*20} History dump {'-'*20}\\n\")\n",
    "    # print(json.dumps(history, indent=2))\n",
    "    # print(f\"\\n{'-'*55}\\n{reset_color}\")\n",
    "\n",
    "    print()\n",
    "    history.append({\"role\": \"user\", \"content\": input(\"> \")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12096353-c3d3-4da9-bba5-a21b127986b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpx\\_transports\\default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpcore\\_sync\\connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpcore\\_sync\\connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpcore\\_sync\\connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m--> 122\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    123\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpcore\\_backends\\sync.py:213\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[1;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[0;32m    212\u001b[0m         sock\u001b[38;5;241m.\u001b[39msetsockopt(\u001b[38;5;241m*\u001b[39moption)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     sock\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mIPPROTO_TCP, socket\u001b[38;5;241m.\u001b[39mTCP_NODELAY, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SyncStream(sock)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    953\u001b[0m         request,\n\u001b[0;32m    954\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    955\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpx\\_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\httpx\\_transports\\default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m      8\u001b[0m history \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an intelligent assistant who specializes in python coding. You always provide the most accurate code in response to queries.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, python code to create a chatbot to communicate with an LLM using Tkinter GUI.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     11\u001b[0m ]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/Phi-3-mini-4k-instruct-gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     new_message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m completion:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\resources\\chat\\completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    605\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_base_client.py:976\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    973\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_base_client.py:976\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    973\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\openai\\_base_client.py:986\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    977\u001b[0m             options,\n\u001b[0;32m    978\u001b[0m             cast_to,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    982\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    983\u001b[0m         )\n\u001b[0;32m    985\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    988\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    990\u001b[0m     request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    995\u001b[0m )\n\u001b[0;32m    996\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx-request-id\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "# Code generation with Phi-3-mini-4k-instruct-gguf model\n",
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant who specializes in python coding. You always provide the most accurate code in response to queries.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, python code to create a chatbot to communicate with an LLM using Tkinter GUI.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "        messages=history,\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    \n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "    history.append(new_message)\n",
    "    \n",
    "    # Uncomment to see chat history\n",
    "    # import json\n",
    "    # gray_color = \"\\033[90m\"\n",
    "    # reset_color = \"\\033[0m\"\n",
    "    # print(f\"{gray_color}\\n{'-'*20} History dump {'-'*20}\\n\")\n",
    "    # print(json.dumps(history, indent=2))\n",
    "    # print(f\"\\n{'-'*55}\\n{reset_color}\")\n",
    "\n",
    "    print()\n",
    "    history.append({\"role\": \"user\", \"content\": input(\"> \")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b33221-c47a-4d3d-8122-c42e119eca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation with meta-llama-8B-instruct.gguf model\n",
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are  a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where is India?.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"moondream/moondream2-gguf\",\n",
    "        messages=history,\n",
    "        temperature=0.3,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    \n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "    history.append(new_message)\n",
    "    \n",
    "    # Uncomment to see chat history\n",
    "    # import json\n",
    "    # gray_color = \"\\033[90m\"\n",
    "    # reset_color = \"\\033[0m\"\n",
    "    # print(f\"{gray_color}\\n{'-'*20} History dump {'-'*20}\\n\")\n",
    "    # print(json.dumps(history, indent=2))\n",
    "    # print(f\"\\n{'-'*55}\\n{reset_color}\")\n",
    "\n",
    "    print()\n",
    "    history.append({\"role\": \"user\", \"content\": input(\"> \")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f41b19-aeae-4231-b3f8-f407e9196797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5419d738-45e5-4a5d-815e-e9efefd06c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp.llama_chat_format import MoondreamChatHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef119b0f-2032-40fa-9697-9ac84a13a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_handler = MoondreamChatHandler(clip_model_path=\"./Models/moondream/moondream2-gguf/moondream2-mmproj-f16.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2f3534-cf41-4366-921d-445033a49c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 245 tensors from ./Models/moondream/moondream2-gguf/moondream2-text-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
      "llama_model_loader: - kv   1:                               general.name str              = moondream2\n",
      "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi2.block_count u32              = 24\n",
      "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - type  f32:  147 tensors\n",
      "llama_model_loader: - type  f16:   98 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 944\n",
      "llm_load_vocab: token to piece cache size = 0.3151 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 51200\n",
      "llm_load_print_meta: n_merges         = 50000\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 32\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.42 B\n",
      "llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) \n",
      "llm_load_print_meta: general.name     = moondream2\n",
      "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 50256 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2706.27 MiB\n",
      "................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   384.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 921\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.architecture': 'phi2', 'phi2.context_length': '2048', 'general.name': 'moondream2', 'phi2.attention.head_count_kv': '32', 'phi2.embedding_length': '2048', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.feed_forward_length': '8192', 'tokenizer.ggml.bos_token_id': '50256', 'phi2.block_count': '24', 'phi2.attention.head_count': '32', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.rope.dimension_count': '32', 'tokenizer.ggml.eos_token_id': '50256', 'general.file_type': '1', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.unknown_token_id': '50256'}\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "  model_path=\"./Models/moondream/moondream2-gguf/moondream2-text-model-f16.gguf\",\n",
    "  chat_handler=chat_handler,\n",
    "  n_ctx=2048, # n_ctx should be increased to accommodate the image embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d41cf06-f807-41fe-a033-96b3be22f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def image_to_base64_data_uri(file_path):\n",
    "    with open(file_path, \"rb\") as img_file:\n",
    "        base64_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        return f\"data:image/png;base64,{base64_data}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9be21235-7e46-4292-a2a3-2dc2fe3c79ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# img = Image.open('./people.jpg')\n",
    "# img.show()\n",
    "data_uri = image_to_base64_data_uri(\"./sale_invoice.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52dc5f4f-73ae-4d0f-9e0d-fe5363e557cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri }},\n",
    "            {\"type\" : \"text\", \"text\": \"What were the grand total sales for chocolate biscuits?\"}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f94ab399-5fdc-4e54-b067-d7975a58c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The grand total"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sales for chocolate biscuits can be seen in the cell of the spreadsheet. This data is essential to understand the popularity and success of these products within their market, allowing companies or organizations to make informed decisions about their product offerings and marketing strategies."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   44938.02 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /    51 runs   (    0.15 ms per token,  6795.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =    2702.41 ms /    51 runs   (   52.99 ms per token,    18.87 tokens per second)\n",
      "llama_print_timings:       total time =    2759.16 ms /    51 tokens\n"
     ]
    }
   ],
   "source": [
    "completion = llm.create_chat_completion(\n",
    "    messages = messages,\n",
    "    temperature = 0.7,\n",
    "    stream = True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    if \"content\" in chunk['choices'][0]['delta']:\n",
    "       print (chunk['choices'][0]['delta']['content'], end = \"\", flush = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48db39b3-114c-457e-8c2a-16f95d37f20d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image is a screenshot of a spreadsheet with various data tables and charts. The main focus is on two columns, one labeled \"Chocolate\" and the other labeled \"Gumballs\". These columns contain lists of numbers representing different types of chocolate and gumballs. In addition to these columns, there are several rows that display more detailed information about the chocolate and gumball quantities. The spreadsheet also includes a chart depicting the relationship between the number of chocolate bars sold and the number of gumballs sold.\n"
     ]
    }
   ],
   "source": [
    "print(completion['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b41e7c-e821-4c26-95a8-c90e22dfb42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e7e1523-b6ab-424e-ba37-1bab13dcf303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./Models/Hermes-2-Pro-Llama-3-8B-GGUF/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Hermes-2-Pro-Llama-3-8B\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128288\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128288]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128288]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128003\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 288\n",
      "llm_load_vocab: token to piece cache size = 0.8007 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128288\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Hermes-2-Pro-Llama-3-8B\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128003 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128003 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.48 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.56 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Hermes-2-Pro-Llama-3-8B', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128003', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128288', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.chat_template': \"{{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India is located in South Asia, specifically on the Indian subcontinent. It is surrounded by the Arabian Sea to the west, the Bay of Bengal to the east, and the Himalayas to the north. The country is bordered by Pakistan, Afghanistan, China, Nepal, Bhutan, Bangladesh, and Myanmar. Its southern region comprises the island of Sri Lanka and the union territory of Lakshadweep."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2034.71 ms\n",
      "llama_print_timings:      sample time =      28.62 ms /    82 runs   (    0.35 ms per token,  2864.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2034.67 ms /    28 tokens (   72.67 ms per token,    13.76 tokens per second)\n",
      "llama_print_timings:        eval time =    6788.58 ms /    81 runs   (   83.81 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =    8978.08 ms /   109 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"./Models/Hermes-2-Pro-Llama-3-8B-GGUF/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\"\n",
    "client = Llama(model_path=model_path)\n",
    "completion = client.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who provides perfect answers.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where is India?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Initialize an empty string to store the assistant's response\n",
    "assistant_response = \"\"\n",
    "\n",
    "for chunk in completion:\n",
    "    if \"content\" in chunk['choices'][0]['delta']:\n",
    "       print (chunk['choices'][0]['delta']['content'], end = \"\", flush = True)\n",
    "# print(assistant_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "000f7c01-439d-4eb5-8a87-6ba441d549c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty string to store the assistant's response\n",
    "assistant_response = \"\"\n",
    "\n",
    "for chunk in completion:\n",
    "    if \"content\" in chunk['choices'][0]['delta']:\n",
    "        assistant_response += chunk['choices'][0]['delta']['content']\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc2f43a-87bd-41e7-a008-6b07457b2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 464 tensors from ./Models/bartowski/gemma-2-9b-it-GGUF/gemma-2-9b-it-Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-9b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 42\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  26:                      quantize.imatrix.file str              = /models/gemma-2-9b-it-GGUF/gemma-2-9b...\n",
      "llama_model_loader: - kv  27:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  28:             quantize.imatrix.entries_count i32              = 294\n",
      "llama_model_loader: - kv  29:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  169 tensors\n",
      "llama_model_loader: - type q4_K:  285 tensors\n",
      "llama_model_loader: - type q5_K:    9 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'gemma2'\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: ./Models/bartowski/gemma-2-9b-it-GGUF/gemma-2-9b-it-Q4_K_S.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Models/bartowski/gemma-2-9b-it-GGUF/gemma-2-9b-it-Q4_K_S.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_chat_completion(\n\u001b[0;32m      6\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      7\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant who provides perfect answers.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize an empty string to store the assistant's response\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\llama_cpp\\llama.py:349\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[1;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_ \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\llama_cpp\\_internals.py:57\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[1;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to load model from file: ./Models/bartowski/gemma-2-9b-it-GGUF/gemma-2-9b-it-Q4_K_S.gguf"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"./Models/bartowski/gemma-2-9b-it-GGUF/gemma-2-9b-it-Q4_K_S.gguf\"\n",
    "client = Llama(model_path=model_path)\n",
    "completion = client.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who provides perfect answers.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where is India?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Initialize an empty string to store the assistant's response\n",
    "assistant_response = \"\"\n",
    "\n",
    "for chunk in completion:\n",
    "    if \"content\" in chunk['choices'][0]['delta']:\n",
    "       print (chunk['choices'][0]['delta']['content'], end = \"\", flush = True)\n",
    "# print(assistant_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586c40d-2757-47df-b2ac-52ff86e0663b",
   "metadata": {},
   "source": [
    "### Implementing Function Calling with Hermes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc0457-914b-4a7e-bd7f-2246ad801283",
   "metadata": {},
   "source": [
    "#### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46c958f6-d178-462d-8807-1810f057b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_weather_forecast(location: str) -> dict[str, str]:\n",
    "    \"\"\"Retrieves the weather forecast for a given location\"\"\"\n",
    "    # Mock values for test\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"forecast\": \"sunny\",\n",
    "        \"temperature\": \"25°C\",\n",
    "    }\n",
    "\n",
    "def get_random_city() -> str:\n",
    "    \"\"\"Retrieves a random city from a list of cities\"\"\"\n",
    "    cities = [\"Groningen\", \"Enschede\", \"Amsterdam\", \"Istanbul\", \"Baghdad\", \"Rio de Janeiro\", \"Tokyo\", \"Kampala\"]\n",
    "    return random.choice(cities)\n",
    "\n",
    "def get_random_number() -> int:\n",
    "    \"\"\"Retrieves a random number\"\"\"\n",
    "    # Mock value for test\n",
    "    return 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85151d27-1c3d-4982-99ef-829bb3c0ec4f",
   "metadata": {},
   "source": [
    "#### 2. Define Function Caller\n",
    "For this example in Jupyter format, We are simply putting the functions in a list. In a python project, you can use the implementation here as an inspiration: https://github.com/AtakanTekparmak/ollama_langhcain_fn_calling/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "878c3c38-7e13-45c3-aebe-3e1dbdf732c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class FunctionCaller:\n",
    "    \"\"\"\n",
    "    A class to call functions from tools.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize the functions dictionary\n",
    "        self.functions = {\n",
    "            \"get_weather_forecast\": get_weather_forecast,\n",
    "            \"get_random_city\": get_random_city,\n",
    "            \"get_random_number\": get_random_number,\n",
    "        }\n",
    "        self.outputs = {}\n",
    "\n",
    "    def create_functions_metadata(self) -> list[dict]:\n",
    "        \"\"\"Creates the functions metadata for the prompt. \"\"\"\n",
    "        def format_type(p_type: str) -> str:\n",
    "            \"\"\"Format the type of the parameter.\"\"\"\n",
    "            # If p_type begins with \"<class\", then it is a class type\n",
    "            if p_type.startswith(\"<class\"):\n",
    "                # Get the class name from the type\n",
    "                p_type = p_type.split(\"'\")[1]\n",
    "            \n",
    "            return p_type\n",
    "            \n",
    "        functions_metadata = []\n",
    "        i = 0\n",
    "        for name, function in self.functions.items():\n",
    "            i += 1\n",
    "            descriptions = function.__doc__.split(\"\\n\")\n",
    "            print(descriptions)\n",
    "            functions_metadata.append({\n",
    "                \"name\": name,\n",
    "                \"description\": descriptions[0],\n",
    "                \"parameters\": {\n",
    "                    \"properties\": [ # Get the parameters for the function\n",
    "                        {   \n",
    "                            \"name\": param_name,\n",
    "                            \"type\": format_type(str(param_type)),\n",
    "                        }\n",
    "                        # Remove the return type from the parameters\n",
    "                        for param_name, param_type in function.__annotations__.items() if param_name != \"return\"\n",
    "                    ],\n",
    "                    \n",
    "                    \"required\": [param_name for param_name in function.__annotations__ if param_name != \"return\"],\n",
    "                } if function.__annotations__ else {},\n",
    "                \"returns\": [\n",
    "                    {\n",
    "                        \"name\": name + \"_output\",\n",
    "                        \"type\": {param_name: format_type(str(param_type)) for param_name, param_type in function.__annotations__.items() if param_name == \"return\"}[\"return\"]\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "\n",
    "        return functions_metadata\n",
    "\n",
    "    def call_function(self, function):\n",
    "        \"\"\"\n",
    "        Call the function from the given input.\n",
    "\n",
    "        Args:\n",
    "            function (dict): A dictionary containing the function details.\n",
    "        \"\"\"\n",
    "    \n",
    "        def check_if_input_is_output(input: dict) -> dict:\n",
    "            \"\"\"Check if the input is an output from a previous function.\"\"\"\n",
    "            for key, value in input.items():\n",
    "                if value in self.outputs:\n",
    "                    input[key] = self.outputs[value]\n",
    "            return input\n",
    "\n",
    "        # Get the function name from the function dictionary\n",
    "        function_name = function[\"name\"]\n",
    "        \n",
    "        # Get the function params from the function dictionary\n",
    "        function_input = function[\"params\"] if \"params\" in function else None\n",
    "        function_input = check_if_input_is_output(function_input) if function_input else None\n",
    "    \n",
    "        # Call the function from tools.py with the given input\n",
    "        # pass all the arguments to the function from the function_input\n",
    "        output = self.functions[function_name](**function_input) if function_input else self.functions[function_name]()\n",
    "        self.outputs[function[\"output\"]] = output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb9d7d-dd57-407c-9033-97b1916ce180",
   "metadata": {},
   "source": [
    "#### Setup The Function Caller and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36f7663e-9426-45f1-bbe0-2893ce018115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retrieves the weather forecast for a given location']\n",
      "['Retrieves a random city from a list of cities']\n",
      "['Retrieves a random number']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the FunctionCaller \n",
    "function_caller = FunctionCaller()\n",
    "\n",
    "# Create the functions metadata\n",
    "functions_metadata = function_caller.create_functions_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "158eba35-8731-4821-ac20-d5d024959323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create the system prompt\n",
    "prompt_beginning = \"\"\"\n",
    "You are an AI assistant that can help the user with a variety of tasks. You have access to the following functions:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_end = \"\"\"\n",
    "\n",
    "When the user asks you a question, if you need to use functions, provide ONLY the function calls, and NOTHING ELSE, in the format:\n",
    "<function_calls>    \n",
    "[\n",
    "    { \"name\": \"function_name_1\", \"params\": { \"param_1\": \"value_1\", \"param_2\": \"value_2\" }, \"output\": \"The output variable name, to be possibly used as input for another function},\n",
    "    { \"name\": \"function_name_2\", \"params\": { \"param_3\": \"value_3\", \"param_4\": \"output_1\"}, \"output\": \"The output variable name, to be possibly used as input for another function\"},\n",
    "    ...\n",
    "]\n",
    "\"\"\"\n",
    "system_prompt = prompt_beginning + f\"<tools> {json.dumps(functions_metadata, indent=4)} </tools>\" + system_prompt_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7debc-5eb8-4ba4-96cf-70371081333d",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a3d5bb-dc0c-4ad6-9b66-f9b5218f4da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "712f9941-5529-4349-9b9b-a0ff3df0991f",
   "metadata": {},
   "source": [
    "## Florence 2- Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d818f5f3-cbcf-425a-bc09-2a34eca41801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OD>': {'bboxes': [[34.23999786376953, 160.0800018310547, 597.4400024414062, 371.7599792480469], [272.32000732421875, 241.67999267578125, 303.67999267578125, 247.4399871826172], [454.0799865722656, 276.7200012207031, 553.9199829101562, 370.79998779296875], [96.31999969482422, 280.55999755859375, 198.0800018310547, 371.2799987792969]], 'labels': ['car', 'door handle', 'wheel', 'wheel']}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM \n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "\n",
    "prompt = \"<OD>\"\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# image.show()\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    num_beams=3,\n",
    ")\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n",
    "print(parsed_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc28ce07-ba73-4667-b0fa-2cfeb1f87828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OD>': {'bboxes': [[34.23999786376953, 160.0800018310547, 597.4400024414062, 371.7599792480469], [272.32000732421875, 241.67999267578125, 303.67999267578125, 247.4399871826172], [454.0799865722656, 276.7200012207031, 553.9199829101562, 370.79998779296875], [96.31999969482422, 280.55999755859375, 198.0800018310547, 371.2799987792969]], 'labels': ['car', 'door handle', 'wheel', 'wheel']}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "def draw_bounding_boxes(image, bboxes, labels):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1 - 10), label, fill=\"red\")\n",
    "    return image\n",
    "\n",
    "def process_image(image_path, is_url=False):\n",
    "    if is_url:\n",
    "        image = Image.open(requests.get(image_path, stream=True).raw)\n",
    "    else:\n",
    "        image = Image.open(image_path)\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n",
    "    \n",
    "    # Draw bounding boxes on the image\n",
    "    result_image = draw_bounding_boxes(image.copy(), parsed_answer['<OD>']['bboxes'], parsed_answer['<OD>']['labels'])\n",
    "    \n",
    "    return result_image, parsed_answer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "prompt = \"<OD>\"\n",
    "\n",
    "# Example usage for URL\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "result_image, parsed_answer = process_image(url, is_url=True)\n",
    "\n",
    "# Example usage for local file\n",
    "# local_path = \"path/to/your/local/image.jpg\"\n",
    "# result_image, parsed_answer = process_image(local_path)\n",
    "\n",
    "result_image.save(\"result_image.jpg\")\n",
    "result_image.show()\n",
    "\n",
    "print(parsed_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7c89d-5600-47d6-aa75-f215800bceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "def draw_bounding_boxes(image, bboxes, labels):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1 - 10), label, fill=\"red\")\n",
    "    return image\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "prompt = \"<OD>\"\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    num_beams=3,\n",
    ")\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n",
    "\n",
    "# Draw bounding boxes on the image\n",
    "result_image = draw_bounding_boxes(image.copy(), parsed_answer['<OD>']['bboxes'], parsed_answer['<OD>']['labels'])\n",
    "\n",
    "# Save or display the result\n",
    "result_image.save(\"result_image.jpg\")\n",
    "result_image.show()\n",
    "\n",
    "print(parsed_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "633f7cea-54e3-4858-b310-acac5009f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OD>': {'bboxes': [[38.13750076293945, 4.612499713897705, 187.08749389648438, 222.6374969482422]], 'labels': ['toy']}}\n",
      "Result image saved to: output_image_with_boxes.jpg\n"
     ]
    }
   ],
   "source": [
    "# image feed\n",
    "\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "def load_image(image_path):\n",
    "    if image_path.startswith(\"http\"):\n",
    "        return Image.open(requests.get(image_path, stream=True).raw)\n",
    "    else:\n",
    "        return Image.open(image_path)\n",
    "\n",
    "def draw_bounding_boxes(image, bboxes, labels):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1 - 10), label, fill=\"red\")\n",
    "    return image\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "\n",
    "# You can change this to a local path, e.g., \"path/to/your/local/image.jpg\"\n",
    "# image_path = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image_path = \"./robot.jpg\"\n",
    "\n",
    "prompt = \"<OD>\"\n",
    "image = load_image(image_path)\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    num_beams=3,\n",
    ")\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n",
    "print(parsed_answer)\n",
    "\n",
    "# Draw bounding boxes on the image\n",
    "result_image = draw_bounding_boxes(image.copy(), parsed_answer['<OD>']['bboxes'], parsed_answer['<OD>']['labels'])\n",
    "\n",
    "# Save the result image\n",
    "output_path = \"output_image_with_boxes.jpg\"\n",
    "result_image.save(output_path)\n",
    "print(f\"Result image saved to: {output_path}\")\n",
    "\n",
    "# Optionally, display the image\n",
    "result_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc9f196-33a4-4cfc-8550-926cc452f11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 frames\n",
      "Processed 20 frames\n",
      "Processed 30 frames\n",
      "Processed 40 frames\n",
      "Processed 50 frames\n",
      "Processed 60 frames\n",
      "Processed 70 frames\n",
      "Processed 80 frames\n",
      "Processed 90 frames\n",
      "Processed 100 frames\n",
      "Processed 110 frames\n",
      "Processed 120 frames\n",
      "Processed 130 frames\n",
      "Processed 140 frames\n",
      "Processed 150 frames\n",
      "Processed 160 frames\n",
      "Processed 170 frames\n",
      "Processed 180 frames\n",
      "Processed 190 frames\n",
      "Processed 200 frames\n",
      "Processed 210 frames\n",
      "Processed 220 frames\n",
      "Processed 230 frames\n",
      "Processed 240 frames\n",
      "Processed 250 frames\n",
      "Processed 260 frames\n",
      "Processed 270 frames\n",
      "Processed 280 frames\n",
      "Processed 290 frames\n",
      "Processed 300 frames\n",
      "Processed 310 frames\n",
      "Processed 320 frames\n",
      "Processed 330 frames\n",
      "Processed video saved to: ./basketball_od.mp4\n"
     ]
    }
   ],
   "source": [
    "# image and video feed\n",
    "\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_image(image_path):\n",
    "    if image_path.startswith(\"http\"):\n",
    "        return Image.open(requests.get(image_path, stream=True).raw)\n",
    "    else:\n",
    "        return Image.open(image_path)\n",
    "\n",
    "def draw_bounding_boxes(image, bboxes, labels):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1 - 10), label, fill=\"red\")\n",
    "    return image\n",
    "\n",
    "def process_frame(frame, model, processor):\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    inputs = processor(text=\"<OD>\", images=pil_image, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(pil_image.width, pil_image.height))\n",
    "    \n",
    "    result_image = draw_bounding_boxes(pil_image.copy(), parsed_answer['<OD>']['bboxes'], parsed_answer['<OD>']['labels'])\n",
    "    return cv2.cvtColor(np.array(result_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def process_media(input_path, output_path, model, processor):\n",
    "    if input_path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "        # Process image\n",
    "        image = load_image(input_path)\n",
    "        inputs = processor(text=\"<OD>\", images=image, return_tensors=\"pt\")\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            num_beams=3,\n",
    "        )\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "        parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n",
    "        result_image = draw_bounding_boxes(image.copy(), parsed_answer['<OD>']['bboxes'], parsed_answer['<OD>']['labels'])\n",
    "        result_image.save(output_path)\n",
    "        print(f\"Processed image saved to: {output_path}\")\n",
    "    \n",
    "    elif input_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "        # Process video\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            processed_frame = process_frame(frame, model, processor)\n",
    "            out.write(processed_frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % 10 == 0:\n",
    "                print(f\"Processed {frame_count} frames\")\n",
    "        \n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Processed video saved to: {output_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Unsupported file format\")\n",
    "\n",
    "# Load model and processor\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "\n",
    "# Example usage\n",
    "input_path = \"./basketball.mp4\"  # Can be an image or video file\n",
    "output_path = \"./basketball_od.mp4\"  # Output will be of the same type as input\n",
    "\n",
    "process_media(input_path, output_path, model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c4fde-a27e-4c60-8602-752c68ba9d5d",
   "metadata": {},
   "source": [
    "# image, video and webcam feed\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def load_image(image_path):\n",
    "    if image_path.startswith(\"http\"):\n",
    "        return Image.open(requests.get(image_path, stream=True).raw)\n",
    "    else:\n",
    "        return Image.open(image_path)\n",
    "\n",
    "def draw_bounding_boxes(image, bboxes, labels):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1 - 10), label, fill=\"red\")\n",
    "    return image\n",
    "\n",
    "def process_frame(frame, model, processor):\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    inputs = processor(text=\"<OD>\", images=pil_image, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(pil_image.width, pil_image.height))\n",
    "    \n",
    "    result_image = draw_bounding_boxes(pil_image.copy(), parsed_answer['<OD>']['bboxes'], parsed_answer['<OD>']['labels'])\n",
    "    return cv2.cvtColor(np.array(result_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def process_media(input_path, output_path, model, processor):\n",
    "    if input_path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "        # Process image\n",
    "        image = load_image(input_path)\n",
    "        inputs = processor(text=\"<OD>\", images=image, return_tensors=\"pt\")\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            num_beams=3,\n",
    "        )\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "        parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n",
    "        result_image = draw_bounding_boxes(image.copy(), parsed_answer['<OD>']['bboxes'], parsed_answer['<OD>']['labels'])\n",
    "        result_image.save(output_path)\n",
    "        print(f\"Processed image saved to: {output_path}\")\n",
    "    \n",
    "    elif input_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "        # Process video\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            processed_frame = process_frame(frame, model, processor)\n",
    "            out.write(processed_frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % 10 == 0:\n",
    "                print(f\"Processed {frame_count} frames\")\n",
    "        \n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Processed video saved to: {output_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Unsupported file format\")\n",
    "\n",
    "def process_webcam(model, processor):\n",
    "    cap = cv2.VideoCapture(0)  # 0 is usually the default webcam\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Processing webcam feed. Press 'q' to quit.\")\n",
    "    \n",
    "    last_process_time = time.time()\n",
    "    process_interval = 1  # Process a frame every 1 second\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Can't receive frame from webcam. Exiting...\")\n",
    "            break\n",
    "        \n",
    "        current_time = time.time()\n",
    "        if current_time - last_process_time >= process_interval:\n",
    "            processed_frame = process_frame(frame, model, processor)\n",
    "            last_process_time = current_time\n",
    "        else:\n",
    "            processed_frame = frame\n",
    "        \n",
    "        cv2.imshow('Webcam Feed', processed_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Load model and processor\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"./Models/Florence-2-base\", trust_remote_code=True)\n",
    "\n",
    "# Choose the mode of operation\n",
    "mode = input(\"Choose mode (image, video, webcam): \").lower()\n",
    "\n",
    "if mode == \"image\" or mode == \"video\":\n",
    "    input_path = input(\"Enter the path to your input file: \")\n",
    "    output_path = input(\"Enter the path for the output file: \")\n",
    "    process_media(input_path, output_path, model, processor)\n",
    "elif mode == \"webcam\":\n",
    "    process_webcam(model, processor)\n",
    "else:\n",
    "    print(\"Invalid mode selected. Please choose 'image', 'video', or 'webcam'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5fcf0-d5f3-4b35-88b3-dd11d051616a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryzenai-transformers",
   "language": "python",
   "name": "ryzenai-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
