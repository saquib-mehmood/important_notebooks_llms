{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1558d35-29c0-4fc5-bdac-ae5f0d90f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PWD=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\ \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET THIRD_PARTY=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET TVM_LIBRARY_PATH=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PATH=C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\Library\\mingw-w64\\bin;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\Library\\usr\\bin;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\Library\\bin;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\Scripts;C:\\Users\\MicroZaib\\miniconda3\\envs\\ryzenai-1\\bin;C:\\Users\\MicroZaib\\miniconda3\\condabin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\JupyterLab;C:\\Users\\MicroZaib\\AppData\\Local\\Programs\\Python\\Python312\\Scripts;C:\\Users\\MicroZaib\\AppData\\Local\\Programs\\Python\\Python312;C:\\Users\\MicroZaib\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\MicroZaib\\miniconda3;C:\\Users\\MicroZaib\\miniconda3\\Scripts;C:\\Users\\MicroZaib\\miniconda3\\Library\\bin;C:\\Users\\MicroZaib\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Windows\\System32;.;C:\\Users\\MicroZaib\\AppData\\Local\\Programs\\Ollama;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\cpp\\;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTORCH_AIE_PATH=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\ \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\python \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\onnx-ops\\python \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\onnx-ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\tools \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>SET PYTHONPATH=;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\lib;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\bin;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\onnx-ops\\python;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\tools;E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\ext\\smoothquant\\smoothquant \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>set XRT_PATH=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\\\third_party\\xrt-ipu \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>set TARGET_DESIGN= \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>set DEVICE=phx \n",
      "\n",
      "(ryzenai-1) E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0>set XLNX_VART_FIRMWARE=E:\\experiments\\ryzenai_chatbot\\Chatbot-with-RyzenAI-1.0\\/xclbin/phx \n"
     ]
    }
   ],
   "source": [
    "#set environment variables after creating environment from the yaml file provided\n",
    "!setup.bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3cfa33-ebdd-4b05-ab72-7cdee6af17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add required system paths\n",
    "import sys\n",
    "sys.path.append(\"../ext\") \n",
    "sys.path.append(\"../smoothquant_package\") \n",
    "sys.path.append(\"./gradio_app\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0004b494-d235-44b1-b6d0-da7bc7aadec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff068bc1-fd9c-4794-a1dd-09a7faf583f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from transformers import set_seed, AutoTokenizer, OPTForCausalLM\n",
    "import pathlib\n",
    "# import smooth\n",
    "import torch\n",
    "import random \n",
    "import string\n",
    "import gradio as gr\n",
    "import time\n",
    "from modeling_ort_amd import ORTModelForCausalLM\n",
    "from smoothquant_package.smooth import smooth_lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90dd1a35-a907-4825-b700-e81851c79658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./opt-1.3b_pretrained_fp32'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to define model path where downloaded pretrained model would be saved\n",
    "model_id = \"facebook/opt-1.3b\" #path to pretrained model on huggingface hub\n",
    "def model_path_fp32(model_id):\n",
    "    model_name = model_id.rsplit('/')[-1]\n",
    "    out_dir = \"./%s_pretrained_fp32\"%model_name\n",
    "    model_path_fp32 = out_dir  \n",
    "    return model_path_fp32\n",
    "\n",
    "model_path_fp32(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e1b1ee-e5c1-4cb2-ba7b-14ba313bf614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and save model from huggingface model hub\n",
    "def download_model(model_id):\n",
    "    \"\"\" Downloads and saves a pretratined model from (e.g. HF) to the specified dir\n",
    "    Args: \n",
    "    model_path_pt - pretrained model path (e.g. facebook/opt-1.3b) \n",
    "    Output: fp-32 model saved in specified dir \"\"\"\n",
    "    \n",
    "    model_name = model_id.rsplit('/')[-1] #extract model name from model path pretrained using python string methods\n",
    "    model = OPTForCausalLM.from_pretrained(model_id)\n",
    "#specify a dir for saving the model\n",
    "    out_dir = \"./%s_pretrained_fp32\"%model_name\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    model.save_pretrained(out_dir)\n",
    "    # print(out_dir)\n",
    "\n",
    "download_model(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c9764-0c44-4db5-8f93-14a6c1eca2ac",
   "metadata": {},
   "source": [
    "- Create activation channel scales for the downloaded modelfor smooth quantization\n",
    "- To be incorporated later from https://github.com/mit-han-lab/smoothquant\n",
    "- Using downloaded act_scales in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9794a123-07f5-4e2a-bcd1-b69bc1cd00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantize the downloaded fp32 model to int8 quantized model after first smooth-quantizing\n",
    "def model_quant(model_path_fp32, model_id):\n",
    "    \"\"\" Quantizes and saves the downloaded fp32 pretratined model to the specified dir, first as smooth quant and then as int8 quantized model\n",
    "    Args: \n",
    "    model_path_fp32 - downloaded fp32 model path  \n",
    "    model_path_pt - pretrained model path (e.g. facebook/opt-1.3b)\n",
    "    Output: smooth quant model saved in specified dir and int-8 quantized model saved in specified dir \"\"\"\n",
    "    \n",
    "    path = model_path_fp32(model_id)\n",
    "    model_name = model_id.rsplit('/')[-1]\n",
    "    if not os.path.exists(path):\n",
    "            print(f\"Pretrained fp32 model not found, exiting..\")\n",
    "            exit(1)\n",
    "    model = OPTForCausalLM.from_pretrained(path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model.tokenizer = tokenizer \n",
    "     # get actuvation scales for smooth quantization\n",
    "    model_act = \"%s.pt\"%model_name\n",
    "    act_scales = torch.load(\"./ext/smoothquant/act_scales/\" + model_act)\n",
    "    # smooth quantize\n",
    "    smooth_lm(model, act_scales, 0.5)\n",
    "    print(model)\n",
    "    # initialize with random prompt\n",
    "    prompt = ''.join(random.choices(string.ascii_lowercase + \" \", k=model.config.max_position_embeddings))\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\")  # takes a lot of time\n",
    "    inputs = tokenizer(\"What is meaning of life\", return_tensors=\"pt\") \n",
    "    print(f\"inputs: {inputs}\")\n",
    "    print(f\"inputs.input_ids: {inputs.input_ids}\")\n",
    "    for key in inputs.keys():\n",
    "        print(inputs[key].shape)\n",
    "        print(inputs[key])\n",
    "    model_out = model(inputs.input_ids)\n",
    "    print(f\"{(model_out.logits.shape)=}\")\n",
    "    out_dir = \"./%s_smoothquant\"%model_name\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # model.save_pretrained(out_dir+\"/model_onnx\")\n",
    "    model.save_pretrained(os.path.join(out_dir, \"model_onnx\"))\n",
    "    print(f\"Saving Smooth Quant fp32 model...\\n \")\n",
    "\n",
    "    print(f\"Quantizing model with Optimum...\\n \")\n",
    "    #proc = subprocess.Popen('cmd.exe', stdin = subprocess.PIPE, stdout = subprocess.PIPE)\n",
    "    # os.system('optimum-cli export onnx -m out_dir\\model_onnx --task text-generation-with-past out_dir\\model_onnx_int8  --framework pt --no-post-process')\n",
    "    os.system(f'optimum-cli export onnx -m {os.path.join(out_dir, \"model_onnx\")} --task text-generation-with-past {os.path.join(out_dir, \"model_onnx_int8\")} --framework pt --no-post-process')\n",
    "\n",
    "    print(f\"Saving quantized int8 model ...\\n \")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8c8936-a152-4645-8275-18fea3268db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49eee892d00b43499d14d2dbe4e52ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
      ")\n",
      "inputs: {'input_ids': tensor([[   2, 2264,   16, 3099,    9,  301]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "inputs.input_ids: tensor([[   2, 2264,   16, 3099,    9,  301]])\n",
      "torch.Size([1, 6])\n",
      "tensor([[   2, 2264,   16, 3099,    9,  301]])\n",
      "torch.Size([1, 6])\n",
      "tensor([[1, 1, 1, 1, 1, 1]])\n",
      "(model_out.logits.shape)=torch.Size([1, 6, 50272])\n",
      "Saving Smooth Quant fp32 model...\n",
      " \n",
      "Quantizing model with Optimum...\n",
      " \n",
      "Saving quantized int8 model ...\n",
      " \n"
     ]
    }
   ],
   "source": [
    "model_quant(model_path_fp32, model_id=\"facebook/opt-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7be7624-2ec6-4064-95a4-c4b5b05e402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"./vaip_config.json\"\n",
    "providers = [\"VitisAIExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "provider_options = [{'config_file': str(config_file_path)}, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5911431c-e9aa-4c41-bd0c-fbc288fdba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(Input_text, max_output_token, model_id = \"facebook/opt-1.3b\", p_type = 0, p_options = 0):\n",
    "    model_name = model_id.rsplit('/')[-1]\n",
    "    out_dir = \"./%s_smoothquant\"%model_name    \n",
    "    onnx_model_path = out_dir+\"/model_onnx_int8\"\n",
    "    model = ORTModelForCausalLM.from_pretrained(onnx_model_path, provider = providers[p_type], provider_options = provider_options[p_options])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    inputs = tokenizer(Input_text, return_tensors=\"pt\") \n",
    "    s = time.perf_counter()\n",
    "    outputs_tkn = model.generate(inputs.input_ids, max_length=max_output_token, use_cache=True, do_sample=False)\n",
    "    e = time.perf_counter() - s\n",
    "    outputs_tkn_len = outputs_tkn.shape[1]\n",
    "    outputs = tokenizer.batch_decode(outputs_tkn,\n",
    "                                    skip_special_tokens=True, \n",
    "                                    clean_up_tokenization_spaces=False)[0]\n",
    "    print(outputs)\n",
    "    yield outputs, \"tkn/sec: \" + str(outputs_tkn_len/e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf916390-d27f-4c71-a795-1d2cdb05015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://localhost:1235\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:1235/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.8.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Couldn't find dll file  libGemmQnnAie_8x2048_2048x2048.dll,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 121855200 3116700 1587800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 107018800 190000 147600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 104910400 226400 137700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 100851100 200800 142400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 101198200 191800 155400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 101519500 200200 145400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 103117300 202800 147000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 105751900 236200 145700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 100429000 199200 139700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 104795400 208500 148600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 101023900 188000 148500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 101375300 204600 136600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 100518300 210500 147500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 99534900 218300 148100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 100616100 200500 150200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 99276900 200600 150100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 107258800 198600 149100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 104590700 201000 153200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 101169000 201700 147000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 120165000 191400 155800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 134398000 296400 261500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 116591000 255600 249700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 107698900 377700 152800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 127713100 260100 151200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 102156700 200900 148000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 106917200 219000 150700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 106291100 197300 515900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 104664500 201900 148800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 103841200 194400 151800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 103653100 206500 148600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 104416200 215000 170300\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 107739600 211300 148100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108074900 212700 149200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 102355600 222900 150000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 118546900 202200 151900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 102937900 212800 147100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 102028900 198900 175800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108666400 207900 150700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 107783500 219800 148500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 109260400 262100 261900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 109389700 218500 151600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 112198800 218200 151600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 125298100 227600 153800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 129579100 251200 164500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 109999700 485100 810500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 113983600 233500 148600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108423800 214500 148900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 109133300 207900 152100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 107989200 222200 148000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108033800 209700 146900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 106247700 217600 146600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 117312500 218400 152400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 112801100 168700 151900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108058000 223500 120600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 107125900 210900 151900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 109140200 205700 152600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108445200 208200 267100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 110727200 198000 151100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108073900 211900 153400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108731700 249700 121600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 112721200 203900 159000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 118564900 200100 396700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 109261800 174000 437900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 110755500 204800 149900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 108963900 214600 155900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 109878400 210600 154500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 107598200 182200 146700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 110507200 201900 148600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 110161900 215800 152700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 125210500 206700 153000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 115205900 512700 281600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 114664700 215800 126400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 113316600 224200 148200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 112001300 204500 152100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 109923300 213600 148000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 111222700 231200 149100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 110786100 217800 149100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 111856100 201700 166900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 120669100 222200 121800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 118201400 235200 156700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 118154100 221500 147900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 115375900 210700 152700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 112577200 177700 148600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 113551200 223000 148600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 114287900 306800 433200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 112989900 203000 151100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 125773200 227400 147100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 111918400 202600 152300\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 114875300 159800 420500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 112854800 166800 262600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 115007100 220200 120500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 116183500 161400 147400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 115615700 226500 147400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 115858400 170400 148200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 117205000 170800 157600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 116831100 257000 129400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 116675500 159200 381800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 117954400 163700 167200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 114613200 170100 146100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 116601000 161400 159500\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 115974300 256900 148400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 117791300 277500 466700\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 115201300 160600 157900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 116203800 173300 123800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 116519300 159200 150100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 117794900 178300 151400\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 119821200 185700 149100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 118218100 287200 402800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 117962400 161000 165800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 119220500 174300 501600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 118625600 162900 150600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 119068100 171200 159100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 117917500 186000 153100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 119039600 168000 145100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 119996300 169600 225100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 120834100 180300 287900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 120840000 164500 477600\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 130526900 170200 428800\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 123664800 282100 344200\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 133417200 169200 142100\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 128190700 288300 122000\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 121704700 179800 389900\n",
      "CRITICAL:root:[PROFILE][ORTAMD] model_decoder_forward 128859900 163900 545600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is India?AprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprAprApr\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=generate_text,\n",
    "    inputs=[\"text\", gr.Slider(minimum=32, maximum=256, value=32, step = 32)],\n",
    "    outputs=[\"text\", \"text\"],\n",
    "    title=\"OPT Chatbot on Ryzen AI with NPU\",\n",
    "    description=\"Simple Chatbot on Ryzen AI Laptop using the NPU\",\n",
    "    concurrency_limit=4\n",
    "    #).queue(concurrency_count=2).launch(server_name=\"localhost\", server_port=1234)\n",
    "    ).queue().launch(server_name=\"localhost\", server_port=1235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b168e3-54bb-47c2-b06c-f7296b9adc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "607d0ebe-5578-4737-b6f7-21cc66b02f3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\torchaudio\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize extension and backend first\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     AudioMetaData,\n\u001b[0;32m      5\u001b[0m     get_audio_backend,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     set_audio_backend,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     compliance,\n\u001b[0;32m     15\u001b[0m     datasets,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     utils,\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\torchaudio\\_extension\\__init__.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m _IS_ALIGN_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[1;32m---> 38\u001b[0m     \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     _check_cuda_version()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\torchaudio\\_extension\\utils.py:60\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\site-packages\\torch\\_ops.py:933\u001b[0m, in \u001b[0;36mload_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    928\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    929\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# let the script frontend know that op is identical to the builtin op\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;66;03m# with qualified_op_name\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_builtins\u001b[38;5;241m.\u001b[39m_register_builtin(op, qualified_op_name)\n\u001b[0;32m    934\u001b[0m op\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m namespace_name\n\u001b[0;32m    935\u001b[0m opoverloadpacket \u001b[38;5;241m=\u001b[39m OpOverloadPacket(\n\u001b[0;32m    936\u001b[0m     qualified_op_name, op_name, op, overload_names\n\u001b[0;32m    937\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ryzenai-transformers\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found"
     ]
    }
   ],
   "source": [
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f0bf9-54eb-41f3-a3f2-5e0e22bbc821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryzenai-transformers",
   "language": "python",
   "name": "ryzenai-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
